[
  {
    "objectID": "docs/demo.html",
    "href": "docs/demo.html",
    "title": "Economic Growth and Gender Equality: An Analysis Using IMF Data",
    "section": "",
    "text": "This data analysis project aims to explore the relationship between economic growth and gender equality using imfp, which allows us to download data from IMF (International Monetary Fund). imfp can be integrated with other python tools to streamline the computational process. To demonstrate its functionality, the project experimented with a variety of visualization and analysis methods."
  },
  {
    "objectID": "docs/demo.html#executive-summary",
    "href": "docs/demo.html#executive-summary",
    "title": "Economic Growth and Gender Equality: An Analysis Using IMF Data",
    "section": "Executive Summary",
    "text": "Executive Summary\nIn this project, we explored the following:\n\nData Fetching\n\n\nMake API call to fetch 4 datasets: GII (Gender Inequality Index), Nominal GDP, GDP Deflator Index, Population series\n\n\nFeature Engineering\n\n\nCleaning: Convert GDP Deflator Index to a yearly basis and variables to numeric\nDependent Variable: Percent Change of Gender Inequality Index\nIndependent Variable: Percent Change of Real GDP per Capita\nTransform variables to display magnitude of change\nMerge the datasets\n\n\nData Visualization\n\n\nScatterplot\nTime Series Line Plots\nBarplot\nBoxplot\nHeatmap\n\n\nStatistical Analysis\n\n\nDescriptive Statistics\nRegression Analysis\nTime Series Analysis"
  },
  {
    "objectID": "docs/demo.html#suggested-packages",
    "href": "docs/demo.html#suggested-packages",
    "title": "Economic Growth and Gender Equality: An Analysis Using IMF Data",
    "section": "Suggested packages",
    "text": "Suggested packages\nThe integration of other Python tools not only streamlined our computational processes but also ensured consistency across the project.\nA custom module is written to simplify the process of making API calls and fetching information with imfp library. load_or_fetch_databases, load_or_fetch_parameters load_or_fetch_dataset load and retreive database, parameters, and dataset from a local or remote source. view_dataframe_in_browser displays dataframe in a web browser.\n\nimport os\nimport pickle\nfrom tempfile import NamedTemporaryFile\nimport pandas as pd\nimport imfp\nimport webbrowser\n\n\n# Function to display a DataFrame in a web browser\ndef view_dataframe_in_browser(df):\n    html = df.to_html()\n    with NamedTemporaryFile(delete=False, mode=\"w\", suffix=\".html\") as f:\n        url = \"file://\" + f.name\n        f.write(html)\n    webbrowser.open(url)\n\n\n# Function to load databases from CSV or fetch from API\ndef load_or_fetch_databases():\n    csv_path = os.path.join(\"data\", \"databases.csv\")\n\n    # Try to load from CSV\n    if os.path.exists(csv_path):\n        try:\n            return pd.read_csv(csv_path)\n        except Exception as e:\n            print(f\"Error loading CSV: {e}\")\n\n    # If CSV doesn't exist or couldn't be loaded, fetch from API\n    print(\"Fetching databases from IMF API...\")\n    databases = imfp.imf_databases()\n\n    # Save to CSV for future use\n    databases.to_csv(csv_path, index=False)\n    print(f\"Databases saved to {csv_path}\")\n\n    return databases\n\n\ndef load_or_fetch_parameters(database_name):\n    pickle_path = os.path.join(\"data\", f\"{database_name}.pickle\")\n\n    # Try to load from pickle file\n    if os.path.exists(pickle_path):\n        try:\n            with open(pickle_path, \"rb\") as f:\n                return pickle.load(f)\n        except Exception as e:\n            print(f\"Error loading pickle file: {e}\")\n\n    # If pickle doesn't exist or couldn't be loaded, fetch from API\n    print(f\"Fetching parameters for {database_name} from IMF API...\")\n    parameters = imfp.imf_parameters(database_name)\n\n    # Save to pickle file for future use\n    os.makedirs(\"data\", exist_ok=True)  # Ensure the data directory exists\n    with open(pickle_path, \"wb\") as f:\n        pickle.dump(parameters, f)\n    print(f\"Parameters saved to {pickle_path}\")\n\n    return parameters\n\n\ndef load_or_fetch_dataset(database_id, indicator):\n    file_name = f\"{database_id}.{indicator}.csv\"\n    csv_path = os.path.join(\"data\", file_name)\n\n    # Try to load from CSV file\n    if os.path.exists(csv_path):\n        try:\n            return pd.read_csv(csv_path)\n        except Exception as e:\n            print(f\"Error loading CSV file: {e}\")\n\n    # If CSV doesn't exist or couldn't be loaded, fetch from API\n    print(f\"Fetching dataset for {database_id}.{indicator} from IMF API...\")\n    dataset = imfp.imf_dataset(database_id=database_id, indicator=[indicator])\n\n    # Save to CSV file for future use\n    os.makedirs(\"data\", exist_ok=True)  # Ensure the data directory exists\n    dataset.to_csv(csv_path, index=False)\n    print(f\"Dataset saved to {csv_path}\")\n\n    return dataset\n\nHere is a brief introduction about the packages used:\npandas: view and manipulate data frame\nmatplotlib.pyplot: make plots\nseaborn: make plots\nnumpy: computation\nLinearRegression: implement linear regression\ntabulate: format data into tables\nstatsmodels.api, adfuller, ARIMA,VAR,plot_acf,plot_pacf,mean_absolute_error,mean_squared_error, andgrangercausalitytests are specifically used for time series analysis.\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\nfrom tabulate import tabulate\nimport statsmodels.api as sm\nfrom statsmodels.tsa.stattools import adfuller\nfrom statsmodels.tsa.arima.model import ARIMA\nfrom statsmodels.tsa.vector_ar.var_model import VAR\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import mean_squared_error\nfrom statsmodels.tsa.stattools import grangercausalitytests"
  },
  {
    "objectID": "docs/demo.html#data-fetching",
    "href": "docs/demo.html#data-fetching",
    "title": "Economic Growth and Gender Equality: An Analysis Using IMF Data",
    "section": "Data Fetching",
    "text": "Data Fetching\nIn this section, we extracted four datasets through API calls: Gender Inequality Index(GII), GDP Deflator, Nominal GDP, and Population.\n\nfrom pathlib import Path\nPath(\"data\").mkdir(exist_ok=True)\n\n\n# Load or fetch databases\ndatabases = load_or_fetch_databases()\n\n# Filter out databases that contain a year in the description\ndatabases[\n  ~databases['description'].str.contains(r\"[\\d]{4}\", regex=True)\n]\n\n# view_dataframe_in_browser(databases)\n\n\n\n\n\n\n\n\ndatabase_id\ndescription\n\n\n\n\n25\nHPDD\nHistorical Public Debt (HPDD)\n\n\n34\nRAFIT2AGG\nRevenue Administration Fiscal Information Tool...\n\n\n47\nGENDER_EQUALITY\nGender Equality\n\n\n63\nPGCS\nPrivate and Public Capital Stock Dataset\n\n\n69\nGENDER_BUDGETING\nGender Budgeting\n\n\n125\nCPI\nConsumer Price Index (CPI)\n\n\n153\nIRFCL\nInternational Reserves and Foreign Currency Li...\n\n\n191\nIFS_DISCONTINUED\nInternational Financial Statistics (IFS), Disc...\n\n\n192\nEQ\nExport Quality\n\n\n193\nED\nExport Diversification\n\n\n226\nFISCALDECENTRALIZATION\nFiscal Decentralization\n\n\n291\nFDI\nFinancial Development Index\n\n\n292\nPSBSFAD\nPublic Sector Balance Sheet (PSBS)(FAD)\n\n\n293\nUNSDG_IMF_INPUTS\nSustainable Development Goals, IMF Inputs\n\n\n294\nCPIS\nCoordinated Portfolio Investment Survey (CPIS)\n\n\n295\nPCTOT\nCommodity Terms of Trade\n\n\n296\nFM\nFiscal Monitor (FM)\n\n\n297\nAFRREO\nSub-Saharan Africa Regional Economic Outlook (...\n\n\n298\nWHDREO\nWestern Hemisphere Regional Economic Outlook (...\n\n\n299\nMCDREO\nMiddle East and Central Asia Regional Economic...\n\n\n300\nAPDREO\nAsia and Pacific Regional Economic Outlook (AP...\n\n\n301\nBOPAGG\nBalance of Payments (BOP), World and Regional ...\n\n\n302\nPCPS\nPrimary Commodity Price System (PCPS)\n\n\n303\nCDIS\nCoordinated Direct Investment Survey (CDIS)\n\n\n304\nBOP\nBalance of Payments (BOP)\n\n\n305\nCOFER\nCurrency Composition of Official Foreign Excha...\n\n\n306\nDOT\nDirection of Trade Statistics (DOTS)\n\n\n307\nRAFIT3P\nRA-FIT Round3 Completion and Participation Rates\n\n\n308\nFAS\nFinancial Access Survey (FAS)\n\n\n309\nFSI\nFinancial Soundness Indicators (FSIs)\n\n\n310\nFSIRE\nFinancial Soundness Indicators: Reporting enti...\n\n\n311\nBOPSDMXUSD\nBalance of Payments (BOP), Global SDMX (US Dol...\n\n\n312\nNAMAIN_IDC_N\nSystem of National Accounts (SNA), NA_MAIN\n\n\n313\nGFSR\nGovernment Finance Statistics (GFS), Revenue\n\n\n314\nGFSSSUC\nGovernment Finance Statistics (GFS), Statement...\n\n\n315\nGFSCOFOG\nGovernment Finance Statistics (GFS), Expenditu...\n\n\n316\nGFSFALCS\nGovernment Finance Statistics (GFS), Financial...\n\n\n317\nGFSIBS\nGovernment Finance Statistics (GFS), Integrate...\n\n\n318\nGFSMAB\nGovernment Finance Statistics (GFS), Main Aggr...\n\n\n319\nGFSE\nGovernment Finance Statistics (GFS), Expense\n\n\n320\nIFS\nInternational Financial Statistics (IFS)\n\n\n321\nMFS\nMonetary and Financial Statistics (MFS)\n\n\n\n\n\n\n\nTwo databases were used: Gender Equality and International Financial Statistics (IFS).\n\ndatasets = [\"GENDER_EQUALITY\", \"IFS\"]\nparams = {}\n\n# Fetch valid parameters for two datasets\nfor dataset in datasets:\n    params[dataset] = load_or_fetch_parameters(dataset)\n\n    valid_keys = list(params[dataset].keys())\n    print(f\"Parameters for {dataset}: \", valid_keys)\n\n# view_dataframe_in_browser(params.get(\"IFS\").get(\"indicator\"))\n\nParameters for GENDER_EQUALITY:  ['freq', 'ref_area', 'indicator']\nParameters for IFS:  ['freq', 'ref_area', 'indicator']\n\n\n“freq” stands for Frequency, such as Annual, Monthly, or Quarterly. “ref_area” stands for Geogrpahical Area, such as US (United States), JP (Japan), and GB (United Kindom). “indicator” refers to the specific dataset in the database, including input code and description. For example, if we display all the indicators for IFS database, the GDP deflator dataset has an input code of “NGDP_D_SA_IX” with a full name description of Gross Domestic Product, Deflator, Seasonally Adjusted, Index.\nWe paired the database with the specific dataset indicator to read and store the csv file.\n\ndatasets = {}\ndsets = [(\"GENDER_EQUALITY\", \"GE_GII\"), (\"IFS\", \"NGDP_D_SA_IX\"), (\"IFS\", \"NGDP_XDC\"), (\"IFS\", \"LP_PE_NUM\")]\n\nfor dset in dsets:\n    datasets[dset[0] + \".\" + dset[1]] = load_or_fetch_dataset(dset[0], dset[1])\n\n\n# \"Gender Inequality Index\"\nGII = \"GENDER_EQUALITY.GE_GII\"\n\n# \"Gross Domestic Product, Deflator, Seasonally Adjusted, Index\"\nGDP_deflator = \"IFS.NGDP_D_SA_IX\"\n\n# \"Gross Domestic Product, Nominal, Domestic Currency\"\nGDP_nominal = \"IFS.NGDP_XDC\"\n\n# \"Population, Persons, Number of\"\nGDP_population = \"IFS.LP_PE_NUM\"\n\n# Assign the datasets to new variables so we don't change the originals\nGII_data = datasets[GII]\nGDP_deflator_data = datasets[GDP_deflator]\nGDP_nominal_data = datasets[GDP_nominal]\nGDP_population_data = datasets[GDP_population]"
  },
  {
    "objectID": "docs/demo.html#feature-engineering",
    "href": "docs/demo.html#feature-engineering",
    "title": "Economic Growth and Gender Equality: An Analysis Using IMF Data",
    "section": "Feature Engineering",
    "text": "Feature Engineering\n\nData Cleaning\nSince the GDP deflator was reported on a quarterly basis, we converted it to a yearly basis.\n\n# Filter to keep only rows with a partial string match for \"Q4\" in the time_period column\nGDP_deflator_data = GDP_deflator_data[GDP_deflator_data['time_period'].str.contains(\"Q4\")]\n\n\n# Split the time_period into year and quarter and keep the year only\nGDP_deflator_data.loc[:, 'time_period'] = GDP_deflator_data['time_period'].str[0:4]\n\nWe made all the variables numeric.\n\ndatasets = [GII_data, GDP_deflator_data, GDP_nominal_data, GDP_population_data]\n\nfor i, dataset in enumerate(datasets):    \n    # Use .loc to modify the columns\n    datasets[i].loc[:, 'obs_value'] = pd.to_numeric(datasets[i]['obs_value'], errors='coerce')\n    datasets[i].loc[:, 'time_period'] = pd.to_numeric(datasets[i]['time_period'], errors='coerce')\n    datasets[i].loc[:, 'unit_mult'] = pd.to_numeric(datasets[i]['unit_mult'], errors='coerce')\n\n\n\nGII Percent Change: Dependent Variable\nWe kept percents as decimals to make them easy to work with for calculation. Different countries have different baseline level of economic growth and gender equality. We calculated the percent change to make them comparable.\n\n# Calculate percent change for each ref_area\n# First, create a copy and reset the index to avoid duplicate index issues\nGII_data_sorted = GII_data.sort_values(['ref_area', 'time_period']).reset_index(drop=True)\nGII_data['pct_change'] = GII_data_sorted.groupby('ref_area')['obs_value'].pct_change()\n\n# Display the first few rows of the updated dataset\nGII_data.head()\n\n\n\n\n\n\n\n\nfreq\nref_area\nindicator\nunit_mult\ntime_format\ntime_period\nobs_value\npct_change\n\n\n\n\n0\nA\nAF\nGE_GII\n0\nP1Y\n1990\n0.828244\nNaN\n\n\n1\nA\nAF\nGE_GII\n0\nP1Y\n1991\n0.817706\n-0.015156\n\n\n2\nA\nAF\nGE_GII\n0\nP1Y\n1992\n0.809806\n-0.016783\n\n\n3\nA\nAF\nGE_GII\n0\nP1Y\n1993\n0.803078\n-0.012651\n\n\n4\nA\nAF\nGE_GII\n0\nP1Y\n1994\n0.797028\n-0.013718\n\n\n\n\n\n\n\nWe subset the data frame to keep only the columns we want:\n\n# Create a new dataframe with only the required columns\nGII_data = GII_data[['ref_area', 'time_period', 'pct_change']].copy()\n\n# Display the first few rows of the new dataset\nGII_data.head()\n\n\n\n\n\n\n\n\nref_area\ntime_period\npct_change\n\n\n\n\n0\nAF\n1990\nNaN\n\n\n1\nAF\n1991\n-0.015156\n\n\n2\nAF\n1992\n-0.016783\n\n\n3\nAF\n1993\n-0.012651\n\n\n4\nAF\n1994\n-0.013718\n\n\n\n\n\n\n\n\n\nGDP: Independent Variable\nWe kept the columns we want only for GDP-related datasets for easier table merging.\n\n# GDP Deflator Dataset\n# Create a new dataframe with only the required columns\nGDP_deflator_data = GDP_deflator_data[['ref_area', 'time_period', 'unit_mult', 'obs_value']].copy()\n\n# Divide obs_value by 100\nGDP_deflator_data['obs_value'] = GDP_deflator_data['obs_value'] / 100\n\n# Display the first few rows of the new dataset\nGDP_deflator_data.head()\n\n\n\n\n\n\n\n\nref_area\ntime_period\nunit_mult\nobs_value\n\n\n\n\n3\nFI\n1990\n0\n0.732006\n\n\n7\nFI\n1991\n0\n0.739841\n\n\n11\nFI\n1992\n0\n0.746543\n\n\n15\nFI\n1993\n0\n0.756193\n\n\n19\nFI\n1994\n0\n0.779373\n\n\n\n\n\n\n\n\n# GDP Nominal Data\n# Create a new dataframe with only the required columns\nGDP_nominal_data = GDP_nominal_data[['ref_area', 'time_period', 'unit_mult','obs_value']].copy()\n\n# Display the first few rows of the new dataset\nGDP_nominal_data.head()\n\n\n\n\n\n\n\n\nref_area\ntime_period\nunit_mult\nobs_value\n\n\n\n\n0\nNE\n2005\n6\n2418864.0\n\n\n1\nNE\n2006\n6\n2596972.0\n\n\n2\nNE\n2007\n6\n2762961.0\n\n\n3\nNE\n2008\n6\n3247835.0\n\n\n4\nNE\n2009\n6\n3403683.0\n\n\n\n\n\n\n\n\n# GDP Population Data \n# Create a new dataframe with only the required columns\nGDP_population_data = GDP_population_data[['ref_area', 'time_period', 'unit_mult','obs_value']].copy()\n\n# Display the first few rows of the new dataset\nGDP_population_data.head()\n\n\n\n\n\n\n\n\nref_area\ntime_period\nunit_mult\nobs_value\n\n\n\n\n0\nGA\n1950\n3\n473.296\n\n\n1\nGA\n1951\n3\n476.381\n\n\n2\nGA\n1952\n3\n478.655\n\n\n3\nGA\n1953\n3\n480.536\n\n\n4\nGA\n1954\n3\n482.332\n\n\n\n\n\n\n\n\n# Combine all the datasets above for further calculation\nmerged_df = pd.merge(pd.merge(GDP_deflator_data,GDP_nominal_data, on=['time_period', 'ref_area'], suffixes=('_index', '_nominal'), how='inner'), GDP_population_data, on=['time_period', 'ref_area'], how='inner')\n\n# Display the GDP data in a web browser\n# view_dataframe_in_browser(merged_df)\n\n# Display the first few rows of the dataset\nmerged_df.head()\n\n\n\n\n\n\n\n\nref_area\ntime_period\nunit_mult_index\nobs_value_index\nunit_mult_nominal\nobs_value_nominal\nunit_mult\nobs_value\n\n\n\n\n0\nFI\n1990\n0\n0.732006\n6\n90964.0\n3\n4996.220\n\n\n1\nFI\n1991\n0\n0.739841\n6\n86913.0\n3\n5019.134\n\n\n2\nFI\n1992\n0\n0.746543\n6\n84786.0\n3\n5044.928\n\n\n3\nFI\n1993\n0\n0.756193\n6\n85610.0\n3\n5071.782\n\n\n4\nFI\n1994\n0\n0.779373\n6\n90646.0\n3\n5097.090\n\n\n\n\n\n\n\nWe want to adjust GDP data based on unit multiplier. Unit multiplier stands for the number of zeroes we need to add to the value column. For example, in 1950, the observed population data for country GA (Georgia) was 473.296. With a unit muliplier of 3, the adjusted population would be 473296.\n\nmerged_df['adjusted_index'] = merged_df['obs_value_index'] * (10 ** (merged_df['unit_mult_index']))\nmerged_df['adjusted_nominal'] = merged_df['obs_value_nominal'] * (10 ** (merged_df['unit_mult_nominal']))\nmerged_df['adjusted_population'] = merged_df['obs_value'] * (10 ** (merged_df['unit_mult']))\n\n\n# Merged dataset\n# Create a new dataframe with only the required columns\nmerged_df = merged_df[['ref_area', 'time_period','adjusted_nominal', 'adjusted_index', 'adjusted_population']].copy()\n\n# Display the first few rows of the dataset\nmerged_df.head()\n\n\n\n\n\n\n\n\nref_area\ntime_period\nadjusted_nominal\nadjusted_index\nadjusted_population\n\n\n\n\n0\nFI\n1990\n9.096400e+10\n0.732006\n4996220.0\n\n\n1\nFI\n1991\n8.691300e+10\n0.739841\n5019134.0\n\n\n2\nFI\n1992\n8.478600e+10\n0.746543\n5044928.0\n\n\n3\nFI\n1993\n8.561000e+10\n0.756193\n5071782.0\n\n\n4\nFI\n1994\n9.064600e+10\n0.779373\n5097090.0\n\n\n\n\n\n\n\nWe wanted to compute the Real GDP per capita.\n\n# Step 1: Real GDP = (Nominal GDP / GDP Deflator Index)\nmerged_df['Real_GDP_domestic'] = (merged_df['adjusted_nominal'] / merged_df['adjusted_index'])\n\n# Step 2: Real GDP per Capita = Real GDP / Population\nmerged_df['Real_GDP_per_capita'] = merged_df['Real_GDP_domestic'] / merged_df['adjusted_population']\n\n# Check the results\nmerged_df.head()\n\n\n\n\n\n\n\n\nref_area\ntime_period\nadjusted_nominal\nadjusted_index\nadjusted_population\nReal_GDP_domestic\nReal_GDP_per_capita\n\n\n\n\n0\nFI\n1990\n9.096400e+10\n0.732006\n4996220.0\n1.242667e+11\n24872.143699\n\n\n1\nFI\n1991\n8.691300e+10\n0.739841\n5019134.0\n1.174753e+11\n23405.490395\n\n\n2\nFI\n1992\n8.478600e+10\n0.746543\n5044928.0\n1.135715e+11\n22512.011198\n\n\n3\nFI\n1993\n8.561000e+10\n0.756193\n5071782.0\n1.132119e+11\n22321.919259\n\n\n4\nFI\n1994\n9.064600e+10\n0.779373\n5097090.0\n1.163063e+11\n22818.181344\n\n\n\n\n\n\n\nWe calculated the percentage change in Real GDP per capita and put it in a new column.\n\n# Calculate percent change for each ref_area\nmerged_df[f'GDP_change'] = merged_df.sort_values(['ref_area', 'time_period']).groupby('ref_area')['Real_GDP_per_capita'].pct_change()\n\n# Rename dataset\nGDP_data = merged_df\n\n# Display the first few rows of the dataset\nGDP_data.head()\n\n\n\n\n\n\n\n\nref_area\ntime_period\nadjusted_nominal\nadjusted_index\nadjusted_population\nReal_GDP_domestic\nReal_GDP_per_capita\nGDP_change\n\n\n\n\n0\nFI\n1990\n9.096400e+10\n0.732006\n4996220.0\n1.242667e+11\n24872.143699\nNaN\n\n\n1\nFI\n1991\n8.691300e+10\n0.739841\n5019134.0\n1.174753e+11\n23405.490395\n-0.058968\n\n\n2\nFI\n1992\n8.478600e+10\n0.746543\n5044928.0\n1.135715e+11\n22512.011198\n-0.038174\n\n\n3\nFI\n1993\n8.561000e+10\n0.756193\n5071782.0\n1.132119e+11\n22321.919259\n-0.008444\n\n\n4\nFI\n1994\n9.064600e+10\n0.779373\n5097090.0\n1.163063e+11\n22818.181344\n0.022232\n\n\n\n\n\n\n\n\n# GII and GDP\n# Merge the datasets\ncombined_data = pd.merge(GII_data, GDP_data, on=[\"ref_area\", \"time_period\"], how = \"inner\")\n\n# Check the combined dataset\ncombined_data.head()\n\n\n\n\n\n\n\n\nref_area\ntime_period\npct_change\nadjusted_nominal\nadjusted_index\nadjusted_population\nReal_GDP_domestic\nReal_GDP_per_capita\nGDP_change\n\n\n\n\n0\nAR\n2004\n-0.031904\n4.851153e+11\n0.409672\n38491970.0\n1.184155e+12\n30763.687776\nNaN\n\n\n1\nAR\n2005\n-0.009444\n5.825382e+11\n0.470108\n38892924.0\n1.239159e+12\n31860.772776\n0.035662\n\n\n2\nAR\n2006\n-0.009401\n7.159043e+11\n0.526535\n39289876.0\n1.359651e+12\n34605.628084\n0.086152\n\n\n3\nAR\n2007\n-0.102661\n8.969801e+11\n0.629139\n39684303.0\n1.425727e+12\n35926.734575\n0.038176\n\n\n4\nAR\n2008\n0.013114\n1.149646e+12\n0.757753\n40080159.0\n1.517178e+12\n37853.600252\n0.053633\n\n\n\n\n\n\n\n\n# GII and GDP\n# Rename columns\ncombined_data = combined_data.rename(columns = {\n    \"ref_area\": \"Country\",\n    \"time_period\": \"Time\",\n    \"pct_change\": \"GII_change\"\n})\n\n# Display the GDP data in a web browser\n# view_dataframe_in_browser(combined_data)\n\n# Check the combined dataset\ncombined_data.head()\n\n\n\n\n\n\n\n\nCountry\nTime\nGII_change\nadjusted_nominal\nadjusted_index\nadjusted_population\nReal_GDP_domestic\nReal_GDP_per_capita\nGDP_change\n\n\n\n\n0\nAR\n2004\n-0.031904\n4.851153e+11\n0.409672\n38491970.0\n1.184155e+12\n30763.687776\nNaN\n\n\n1\nAR\n2005\n-0.009444\n5.825382e+11\n0.470108\n38892924.0\n1.239159e+12\n31860.772776\n0.035662\n\n\n2\nAR\n2006\n-0.009401\n7.159043e+11\n0.526535\n39289876.0\n1.359651e+12\n34605.628084\n0.086152\n\n\n3\nAR\n2007\n-0.102661\n8.969801e+11\n0.629139\n39684303.0\n1.425727e+12\n35926.734575\n0.038176\n\n\n4\nAR\n2008\n0.013114\n1.149646e+12\n0.757753\n40080159.0\n1.517178e+12\n37853.600252\n0.053633"
  },
  {
    "objectID": "docs/demo.html#data-visualization",
    "href": "docs/demo.html#data-visualization",
    "title": "Economic Growth and Gender Equality: An Analysis Using IMF Data",
    "section": "Data Visualization",
    "text": "Data Visualization\n\nScatterplot\nScatterplot use dots to represent values of two numeric variables. The horizontal axis was the percent change in Real GDP per capita. The vertical axis was the percent change in Gender Inequality Index(GII). Different colors represented different countries. We used a linear regression line to display the overall pattern.\nBased on the scatterplot, it seemed like there was a slight positive relationship between GDP change and GII change as shown by the flat regression line. Gender inequality was decreasing (gender equality was improving) a little faster in country-years with low GDP growth and a little slower in country-years with high GDP growth.\n\n# Drop NAs\ncombined_data = combined_data.dropna(subset=['GDP_change', 'GII_change'])\n\n# Plot the data points\nplt.figure(figsize=(18, 10))\nfor country in combined_data['Country'].unique():\n    country_data = combined_data[combined_data['Country'] == country]\n    plt.scatter(country_data['GDP_change'], country_data['GII_change'],\n             marker='o',linestyle='-', label=country)\nplt.title('Country-Year Analysis of GDP Change vs. GII Change')\nplt.xlabel('Percent Change in Real GDP per Capita (Country-Year)')\nplt.ylabel('Percent Change in GII (Country-Year)')\nplt.grid(True)\n\n# Prepare data for linear regression\nX = combined_data['GDP_change'].values.reshape(-1, 1)\ny = combined_data['GII_change'].values\n\n# Perform linear regression\nreg = LinearRegression().fit(X, y)\ny_pred = reg.predict(X)\n\n# Plot the regression line\nplt.plot(combined_data['GDP_change'], y_pred, color='red', linewidth=2)\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\nTime Series Line Plot\nWe created separate line plots for GDP change and GII change over time for a few key countries might show the trends more clearly.\nUS: United States\nJP: Japan\nGB: United Kindom\nFR: France\nMX: Mexico\nBased on the line plots, we saw GDP change and GII change have different patterns. For example, in Mexico, when there was a big change in real GDP per captia in 1995, the change in GII was pretty stable.\n\n# Time Series Line plot for a few key countries\nselected_countries  = ['US', 'JP', 'GB', 'FR', 'MX']\ncombined_data_selected = combined_data[combined_data['Country'].isin(selected_countries)]\n\n# Set up the Plot Structure\nfig, ax = plt.subplots(2, 1, figsize=(18, 10), sharex=True)\n\n# Plot change in real GDP per capita over time\nsns.lineplot(data = combined_data_selected, x = \"Time\", y = \"GDP_change\", hue = \"Country\", ax = ax[0])\nax[0].set_title(\"Percent Change in Real GDP per Capita Over Time\")\nax[0].set_ylabel(\"Percent Change in Real GDP per Capita\")\n\n# Plot change in GII over time\nsns.lineplot(data = combined_data_selected, x = \"Time\", y = \"GII_change\", hue = \"Country\", ax = ax[1])\nax[1].set_title(\"Percent Change in GII over Time\")\nax[1].set_xlabel(\"Time\")\nax[1].set_ylabel(\"GII\")\n\nplt.tight_layout\nplt.show()\n\n\n\n\n\n\n\n\n\n\nBarplot\nWe used a barplot to show average changes in GII and GDP percent change for each country to visualize regions where inequality was improving or worsening.\nThis plot supported our previous observation how GII change seemed to be not be correlated with GDP change. We also saw that, for country SI, Solvenia, there seems to be a large improvement in gender inequality.\n\n# Barplot using average GII and GDP change\n# Calculate average change for each country\ncombined_data_avg = combined_data.groupby('Country')[['GII_change','GDP_change']].mean().reset_index()\n\n# Prepare to plot structure \nplt.figure(figsize = (18,10))\n\n# Create the barplot\ncombined_data_avg.plot(kind = 'bar', x = 'Country')\nplt.ylabel('Average Change')\nplt.xlabel('Country')\nplt.legend(['GII change', 'GDP change'])\nplt.grid(axis = 'y')\n\n# Show the plot\nplt.show()\n\n&lt;Figure size 1728x960 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n\n\nBoxplot\nWe used boxplot to visualize the distribution of GDP and GII change by country, providing information about spread, median, and potential outliers. To provide a more informative view, we sequenced countries in an ascending order by the median of percent change in GDP.\nThe boxplot displayed a slight upward trend with no obvious pattern between GDP and GII change. In coutries with higher GDP change median, they also tend to have a larger spread of the GDP change. The median of GII change remained stable regardless of the magnitude of GDP change, implying weak or no association between GDP and GII change. We observed a potential outlier for country SI, Solvenia, which may explained its large improvement in Gender inequality.\n\n# Box plot for GII and GDP change\n# Melt the dataframe to long format for combined boxplot\ncombined_data_melted = combined_data.melt(id_vars=['Country'], value_vars=['GII_change', 'GDP_change'], \n                                          var_name='Change_Type', value_name='Value')\n\ngdp_medians = combined_data.groupby('Country')['GDP_change'].median().sort_values()\n\ncombined_data_melted['Country'] = pd.Categorical(combined_data_melted['Country'], categories=gdp_medians.index, ordered= True)\n\n# Prepare the plot structure\nplt.figure(figsize=(18, 10))\nsns.boxplot(data = combined_data_melted, x = \"Country\", y = 'Value', hue = 'Change_Type')\nplt.title('Distribution of GII and GDP change by Country')\nplt.xlabel('Country')\nplt.ylabel('Change')\nplt.legend(title = 'Change Type')\n\n# Show the plot\nplt.show()\n\n\n\n\n\n\n\n\n\n\nCorrelation Matrix\nWe created a heatmap to show the relationship between GII and GDP change.\nA positive correlation coefficient indicates a positive relationship: the larger the GDP change, the larger the GII change. A negative correlation coefficient indicates a negative relationship: the larger the GDP change, the smaller the GII change. A correlation coefficient closer to 0 indicates there is weak or no relationship.\nBased on the numeric values in the plot, there was a moderately strong positive correlation between GII and GDP change for country Estonia(EE) and Ireland(IE).\n\n# Calculate the correlation\ncountry_correlation = combined_data.groupby('Country')[['GII_change', 'GDP_change']].corr().iloc[0::2, -1].reset_index(name='Correlation')\n\n# Put the correlation value in a matrix format\ncorrelation_matrix = country_correlation.pivot(index='Country', columns='level_1', values='Correlation')\n\n# Check for NaN values in the correlation matrix\ncorrelation_matrix.fillna(0, inplace=True)  # Replace NaNs with 0 or another value as appropriate\n\n# Set up the plot structure\nplt.figure(figsize=(18, 12))  # Adjust height to give more space for y-axis labels\n\n# Plot the heatmap\nsns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, cbar_kws={\"shrink\": .8}, linewidths=.5)\n\n# Enhance axis labels and title\nplt.title('Heatmap for GII and GDP Change', fontsize=20)\nplt.xlabel('Variables', fontsize=16)\nplt.ylabel('Country', fontsize=16)\n\n# Improve readability of y-axis labels\nplt.yticks(fontsize=12)  # Adjust the font size for y-axis labels\n\n# Show the plot\nplt.show()"
  },
  {
    "objectID": "docs/demo.html#statistical-analysis",
    "href": "docs/demo.html#statistical-analysis",
    "title": "Economic Growth and Gender Equality: An Analysis Using IMF Data",
    "section": "Statistical Analysis",
    "text": "Statistical Analysis\n\nDescriptive Statistics\nThere was a total of 915 data points. The mean of the GII change in -0.0314868, which indicated the overall grand mean percent change in gender inequality index is -3.15%. The mean of the GDP change was 0.0234633, showing the overall grand mean percent change in real GDP per capita was 2.35%.\n\n# Generate summary statistics\ndescriptive_stats = combined_data.describe()\nprint(tabulate(descriptive_stats, headers='keys', tablefmt='github'))\n\n|       |   GII_change |   adjusted_nominal |   adjusted_index |   adjusted_population |   Real_GDP_domestic |   Real_GDP_per_capita |   GDP_change |\n|-------|--------------|--------------------|------------------|-----------------------|---------------------|-----------------------|--------------|\n| count | 892          |      892           |      892         |         892           |       892           |         892           | 892          |\n| mean  |  -0.0217928  |        7.87576e+13 |        0.852849  |           4.52596e+07 |         7.6152e+13  |      994449           |   0.023761   |\n| std   |   0.0413772  |        6.07694e+14 |        0.211413  |           1.25789e+08 |         5.5827e+14  |           3.86259e+06 |   0.0420188  |\n| min   |  -0.552535   |        2.18714e+09 |        0.0360636 |      396324           |         5.21733e+09 |        1798.42        |  -0.285847   |\n| 25%   |  -0.0303228  |        1.08215e+11 |        0.73791   |           5.09268e+06 |         1.48253e+11 |       17173.4         |   0.00414482 |\n| 50%   |  -0.0112739  |        7.55228e+11 |        0.886025  |           1.03354e+07 |         1.00125e+12 |       35429.4         |   0.0222486  |\n| 75%   |  -0.00354882 |        2.51415e+12 |        1.00091   |           4.48045e+07 |         2.92126e+12 |      120734           |   0.0432811  |\n| max   |   0.210491   |        9.54613e+15 |        2.0788    |           1.28084e+09 |         7.92007e+15 |           3.14532e+07 |   0.241984   |\n\n\n\n\nRegression Analysis\nSimple linear regression as a foundational approach provide us with a basic understanding of the relationship between GDP change and GII change.\nBased on the summary, we concluded the following:\n\nBecasue p-value = 0.057, if we set alpha, the significance level, to be 0.05, we failed to reject the null hypothesis and conclude there was no significant relationship between percent change in real GDP per capita and gender inequality index.\nR-squared = 0.004. Only 0.4% of the variance in GII change could be explained by GDP change.\nWe were 95% confident that the interval from -0.003 to 0.169 captured the true slope of GDP change. Because 0 was included, we are uncertain about the effect of GDP change on GII chnage.\n\n\n# Define independent and depenent variables\nX = combined_data['GDP_change']\ny = combined_data['GII_change']\n\n# Add a constant to indepdent variable to include an intercept\nX = sm.add_constant(X)\n\n# Fit a simple linear regresion model and print out the summary\nmodel = sm.OLS(y, X).fit()\nprint(model.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:             GII_change   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                 -0.001\nMethod:                 Least Squares   F-statistic:                    0.1065\nDate:                Fri, 27 Dec 2024   Prob (F-statistic):              0.744\nTime:                        17:39:33   Log-Likelihood:                 1575.9\nNo. Observations:                 892   AIC:                            -3148.\nDf Residuals:                     890   BIC:                            -3138.\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst         -0.0220      0.002    -13.845      0.000      -0.025      -0.019\nGDP_change     0.0108      0.033      0.326      0.744      -0.054       0.076\n==============================================================================\nOmnibus:                      867.158   Durbin-Watson:                   1.571\nProb(Omnibus):                  0.000   Jarque-Bera (JB):            61900.365\nSkew:                          -4.267   Prob(JB):                         0.00\nKurtosis:                      42.908   Cond. No.                         23.8\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n\nTime Series Analysis\nTime series analysis allows us to explore how the relationship between GII and GDP change vary across different time periods, accounting for lagged effects.\nHere was a quick summary of the result: * Both GII and GDP change time series were stationary.\n\nPast GII change values significantly influenced cuurent GII change values.\nVAR model had good model performance on forecasting future values based on historical data.\nChanges in GDP did not cause/precde the changes in GII.\n\n\nADF Test: Stationality Assumption Check\nWe wanted to use Augmented Dickey-Fuller (ADF) test to check whether a time series was stationary, which was the model assumption for many time series models.\nStationarity implied constant mean and variance over time, making it more predictable and stable for forecasting.\nBased on the ADF test output, both GII and GDP change time series were stationary. We proceeded to the time series modeling section.\n\n# Augmented Dickey-Fuller (ADF) test for stationarity check\n# Create melted datasets\ncombined_data_time = combined_data.melt(id_vars=['Time', 'Country'], value_vars=['GII_change','GDP_change'], var_name = 'Change_Type', value_name = 'Value')\nGII = combined_data_time[(combined_data_time['Change_Type'] == 'GII_change')]                         \n\nGDP = combined_data_time[(combined_data_time['Change_Type'] == 'GDP_change')]\n\n# Stationary Check\ndef adf_test(series):\n    result = adfuller(series.dropna())\n    print(f'ADF Statistic: {result[0]}')\n    print(f'p-value: {result[1]}')\n    if result[1] &lt; 0.05:\n        print(\"Series is stationary\")\n    else:\n        print(\"Series is not stationary\")\n\n# Output the result\nadf_test(GII['Value'])\nadf_test(GDP['Value'])\n\nADF Statistic: -13.235165843988412\np-value: 9.410722393209802e-25\nSeries is stationary\nADF Statistic: -13.53593861735856\np-value: 2.567675334076584e-25\nSeries is stationary\n\n\n\n\nVAR model: Examine variables separately\nWe fitted a VAR (Vector Autoreression) model to see the relationship between GII and GDP change. VAR is particularly useful when dealing with multivariate time series data and allows us to examine the interdependence between variables.\nBased on summary, here were several interpretations we could make:\n\nWe used AIC as the criteria for model selection. Lower value suggests a better fit.\nGiven that we wanted to predict GII change, we focused on the first set “Results for equation GII_change.”\nPast GII_change values significantly influenced current GII_change, as shown in the small p-values of lags 1 and 2.\nLag 2 of GDP_change had a relatively low p-value but is not statistically significant.\n\n\n# Split the dataset into training and testing sets\nsplit_ratio = 0.7\nsplit_index = int(len(combined_data) * split_ratio)\n\n# Training set is used to fit the model\ntrain_data = combined_data.iloc[:split_index]\n\n# Testing set is used for validation\ntest_data = combined_data.iloc[split_index:]\n\nprint(f\"Training data: {train_data.shape}\")\nprint(f\"Test data: {test_data.shape}\")\n\nTraining data: (624, 9)\nTest data: (268, 9)\n\n\n\n# Fit a VAR model \ntime_model = VAR(train_data[['GII_change', 'GDP_change']])\ntime_model_fitted = time_model.fit(maxlags = 15, ic=\"aic\")\n\n# Print out the model summary\nprint(time_model_fitted.summary())\n\n  Summary of Regression Results   \n==================================\nModel:                         VAR\nMethod:                        OLS\nDate:           Fri, 27, Dec, 2024\nTime:                     17:39:34\n--------------------------------------------------------------------\nNo. of Equations:         2.00000    BIC:                   -12.6392\nNobs:                     621.000    HQIC:                  -12.7003\nLog likelihood:           2207.18    FPE:                2.93407e-06\nAIC:                     -12.7391    Det(Omega_mle):     2.86903e-06\n--------------------------------------------------------------------\nResults for equation GII_change\n================================================================================\n                   coefficient       std. error           t-stat            prob\n--------------------------------------------------------------------------------\nconst                -0.014365         0.002526           -5.686           0.000\nL1.GII_change         0.206883         0.040090            5.160           0.000\nL1.GDP_change         0.013287         0.037681            0.353           0.724\nL2.GII_change         0.147726         0.040493            3.648           0.000\nL2.GDP_change        -0.037889         0.037687           -1.005           0.315\nL3.GII_change         0.071916         0.040325            1.783           0.075\nL3.GDP_change         0.041419         0.037418            1.107           0.268\n================================================================================\n\nResults for equation GDP_change\n================================================================================\n                   coefficient       std. error           t-stat            prob\n--------------------------------------------------------------------------------\nconst                 0.017777         0.002672            6.652           0.000\nL1.GII_change        -0.022926         0.042407           -0.541           0.589\nL1.GDP_change         0.131506         0.039859            3.299           0.001\nL2.GII_change         0.124323         0.042833            2.902           0.004\nL2.GDP_change         0.025850         0.039866            0.648           0.517\nL3.GII_change        -0.063846         0.042655           -1.497           0.134\nL3.GDP_change         0.146124         0.039581            3.692           0.000\n================================================================================\n\nCorrelation matrix of residuals\n              GII_change  GDP_change\nGII_change      1.000000   -0.034159\nGDP_change     -0.034159    1.000000\n\n\n\n\n\n/home/chriscarrollsmith/.cache/pypoetry/virtualenvs/imfp-z9RgynLW-py3.12/lib/python3.12/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: An unsupported index was provided. As a result, forecasts cannot be generated. To use the model for forecasting, use one of the supported classes of index.\n  self._init_dates(dates, freq)\n\n\n\n\nVAR Model: Forecasting\nWe applied the model learned above to the test data. Based on the plot, the forecast values seem to follow the actual data well, indicating a good model fit caputuring the underlying trends.\n\n# Number of steps to forecast (length of the test set)\nn_steps = len(test_data)\n\n# Get the last values from the training set for forecasting\nforecast_input = train_data[['GII_change', 'GDP_change']].values[-time_model_fitted.k_ar:]\n\n# Forecasting\nforecast = time_model_fitted.forecast(y=forecast_input, steps=n_steps)\n\n# Create a DataFrame for the forecasted values\nforecast_df = pd.DataFrame(forecast, index=test_data.index, columns=['GII_forecast', 'GDP_forecast'])\n\n# Ensure the index of the forecast_df matches the test_data index\nforecast_df.index = test_data.index\n\n\nplt.figure(figsize=(12, 6))\nplt.plot(train_data['GII_change'], label='Training GII', color='blue')\nplt.plot(test_data['GII_change'], label='Actual GII', color='orange')\nplt.plot(forecast_df['GII_forecast'], label='Forecasted GII', color='green')\nplt.title('GII Change Forecast vs Actual')\nplt.legend()\nplt.show()\n\nplt.figure(figsize=(12, 6))\nplt.plot(train_data['GDP_change'], label='Training GDP', color='blue')\nplt.plot(test_data['GDP_change'], label='Actual GDP', color='orange')\nplt.plot(forecast_df['GDP_forecast'], label='Forecasted GDP', color='green')\nplt.title('GDP Change Forecast vs Actual')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVAR Model: Model Performance\nLow values of both MAE and RMSE indicate good model performance with small average errors in predictions.\n\nmae_gii = mean_absolute_error(test_data['GII_change'], forecast_df['GII_forecast'])\nmae_gdp = mean_absolute_error(test_data['GDP_change'], forecast_df['GDP_forecast'])\n\nprint(f'Mean Absolute Error for GII: {mae_gii}')\nprint(f'Mean Absolute Error for GDP: {mae_gdp}')\n\nMean Absolute Error for GII: 0.02161276041747538\nMean Absolute Error for GDP: 0.027925275568838715\n\n\n\nrmse_gii = np.sqrt(mean_squared_error(test_data['GII_change'], forecast_df['GII_forecast']))\nrmse_gdp = np.sqrt(mean_squared_error(test_data['GDP_change'], forecast_df['GDP_forecast']))\n\nprint(f'RMSE for GII: {rmse_gii}')\nprint(f'RMSE for GDP: {rmse_gdp}')\n\nRMSE for GII: 0.040120750620236574\nRMSE for GDP: 0.03873101325357032\n\n\n\n\nVAR Model: Granger causality test\nGranger causality test evaluates whether one time series can predict another.\nBased on the output, the lowest p-value is when lag = 2. However, because p-value &gt; 0.05, we fail to reject the null hypothesis and conclude the GDP_change does not Granger-cause the GII_change.\n\n# Perform the Granger causality test\nmax_lag = 3\ntest_result = grangercausalitytests(train_data[['GII_change', 'GDP_change']], max_lag, verbose=True)\n\n\nGranger Causality\nnumber of lags (no zero) 1\nssr based F test:         F=0.1097  , p=0.7406  , df_denom=620, df_num=1\nssr based chi2 test:   chi2=0.1102  , p=0.7399  , df=1\nlikelihood ratio test: chi2=0.1102  , p=0.7399  , df=1\nparameter F test:         F=0.1097  , p=0.7406  , df_denom=620, df_num=1\n\nGranger Causality\nnumber of lags (no zero) 2\nssr based F test:         F=0.4806  , p=0.6186  , df_denom=617, df_num=2\nssr based chi2 test:   chi2=0.9690  , p=0.6160  , df=2\nlikelihood ratio test: chi2=0.9682  , p=0.6162  , df=2\nparameter F test:         F=0.4806  , p=0.6186  , df_denom=617, df_num=2\n\nGranger Causality\nnumber of lags (no zero) 3\nssr based F test:         F=0.6864  , p=0.5606  , df_denom=614, df_num=3\nssr based chi2 test:   chi2=2.0827  , p=0.5554  , df=3\nlikelihood ratio test: chi2=2.0793  , p=0.5561  , df=3\nparameter F test:         F=0.6864  , p=0.5606  , df_denom=614, df_num=3\n\n\n/home/chriscarrollsmith/.cache/pypoetry/virtualenvs/imfp-z9RgynLW-py3.12/lib/python3.12/site-packages/statsmodels/tsa/stattools.py:1556: FutureWarning: verbose is deprecated since functions should not print results\n  warnings.warn("
  },
  {
    "objectID": "docs/demo.html#conclusion",
    "href": "docs/demo.html#conclusion",
    "title": "Economic Growth and Gender Equality: An Analysis Using IMF Data",
    "section": "Conclusion",
    "text": "Conclusion\nIn wrapping up our analysis, we found no evidence to support a significant relationship between the Change in Real GDP per capita and the Change in the Gender Inequality Index (GII). This suggests that economic growth may not have a direct impact on gender equality. However, our findings open the door to questions for future research. Check out the blog for full analysis!"
  },
  {
    "objectID": "docs/demo.html#about-the-author",
    "href": "docs/demo.html#about-the-author",
    "title": "Economic Growth and Gender Equality: An Analysis Using IMF Data",
    "section": "About the Author",
    "text": "About the Author\n\n\n\nHi there! My name is Jenny, and I’m a third-year student at University of California, Davis, double majoring in Statistics and Psychology. I’ve always been interested in becoming a data analyst working in tech, internet, or research industries. Interning at Promptly Technologies helped me learn a ton. A quick fun fact for me is that my MBTI is ISFJ (Defender)!\n\n  Email    LinkedIn    GitHub"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "imfp",
    "section": "",
    "text": "imfp, by Christopher C. Smith, is a Python package for downloading data from the International Monetary Fund’s RESTful JSON API.\n\n\nTo install the stable version of imfp from PyPi, use pip.\npip install -q --upgrade imfp\nTo load the library, use import:\n\nimport imfp\n\n\n\n\n\n\nimfp outputs data in a pandas data frame, so you will want to use the pandas package for its functions for viewing and manipulating this object type. I also recommend matplotlib or seaborn for making plots, and numpy for computation. These packages can be installed using pip and loaded using import:\npip install -q pandas matplotlib seaborn numpy\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n\n\n\n\nThe imfp package introduces four core functions: imfp.imf_databases, imfp.imf_parameters, imfp.imf_parameter_defs, and imfp.imf_dataset. The function for downloading datasets is imfp.imf_dataset, but you will need the other functions to determine what arguments to supply to imfp.imf_dataset. For instance, all calls to imfp.imf_dataset require a database_id. This is because the IMF serves many different databases through its API, and the API needs to know which of these many databases you’re requesting data from. To obtain a list of databases, use imfp.imf_databases, like so:\n\n#Fetch the list of databases available through the IMF API\ndatabases = imfp.imf_databases()\ndatabases.head()\n\n\n\n\n\n\n\n\ndatabase_id\ndescription\n\n\n\n\n0\nBOP_2017M06\nBalance of Payments (BOP), 2017 M06\n\n\n1\nBOP_2020M3\nBalance of Payments (BOP), 2020 M03\n\n\n2\nBOP_2017M11\nBalance of Payments (BOP), 2017 M11\n\n\n3\nDOT_2020Q1\nDirection of Trade Statistics (DOTS), 2020 Q1\n\n\n4\nGFSMAB2016\nGovernment Finance Statistics Yearbook (GFSY 2...\n\n\n\n\n\n\n\nThis function returns the IMF’s listing of 259 databases available through the API. (In reality, 8 of the listed databases are defunct and not actually available: FAS_2015, GFS01, FM202010, APDREO202010, AFRREO202010, WHDREO202010, BOPAGG_2020, DOT_2020Q1.)\nTo view and explore the database list, it’s possible to explore subsets of the data frame by row number with databases.loc:\n\n# View a subset consisting of rows 5 through 9\ndatabases.loc[5:9]\n\n\n\n\n\n\n\n\ndatabase_id\ndescription\n\n\n\n\n5\nBOP_2019M12\nBalance of Payments (BOP), 2019 M12\n\n\n6\nGFSYFALCS2014\nGovernment Finance Statistics Yearbook (GFSY 2...\n\n\n7\nGFSE2016\nGovernment Finance Statistics Yearbook (GFSY 2...\n\n\n8\nFM201510\nFiscal Monitor (FM) October 2015\n\n\n9\nGFSIBS2016\nGovernment Finance Statistics Yearbook (GFSY 2...\n\n\n\n\n\n\n\nOr, if you already know which database you want, you can fetch the corresponding code by searching for a string match using str.contains and subsetting the data frame for matching rows. For instance, here’s how to search for commodities data:\n\ndatabases[databases['description'].str.contains(\"Commodity\")]\n\n\n\n\n\n\n\n\ndatabase_id\ndescription\n\n\n\n\n295\nPCTOT\nCommodity Terms of Trade\n\n\n302\nPCPS\nPrimary Commodity Price System (PCPS)\n\n\n\n\n\n\n\n\n\n\nOnce you have a database_id, it’s possible to make a call to imfp.imf_dataset to fetch the entire database: imfp.imf_dataset(database_id). However, while this will succeed for a few small databases, it will fail for all of the larger ones. And even in the rare case when it succeeds, fetching an entire database can take a long time. You’re much better off supplying additional filter parameters to reduce the size of your request.\nRequests to databases available through the IMF API are complicated by the fact that each database uses a different set of parameters when making a request. (At last count, there were 43 unique parameters used in making API requests from the various databases!) You also have to have the list of valid input codes for each parameter. The imfp.imf_parameters function solves this problem. Use the function to obtain the full list of parameters and valid input codes for a given database:\n\n# Fetch list of valid parameters and input codes for commodity price database\nparams = imfp.imf_parameters(\"PCPS\")\n\nThe imfp.imf_parameters function returns a dictionary of data frames. Each dictionary key name corresponds to a parameter used in making requests from the database:\n\n# Get key names from the params object\nparams.keys()\n\ndict_keys(['freq', 'ref_area', 'commodity', 'unit_measure'])\n\n\nIn the event that a parameter name is not self-explanatory, the imfp.imf_parameter_defs function can be used to fetch short text descriptions of each parameter:\n\n# Fetch and display parameter text descriptions for the commodity price database\nimfp.imf_parameter_defs(\"PCPS\")\n\n\n\n\n\n\n\n\nparameter\ndescription\n\n\n\n\n0\nfreq\nFrequency\n\n\n1\nref_area\nGeographical Areas\n\n\n2\ncommodity\nIndicator\n\n\n3\nunit_measure\nUnit\n\n\n\n\n\n\n\nEach named list item is a data frame containing a vector of valid input codes that can be used with the named parameter, and a vector of text descriptions of what each code represents.\nTo access the data frame containing valid values for each parameter, subset the params dict by the parameter name:\n\n# View the data frame of valid input codes for the frequency parameter\nparams['freq']\n\n\n\n\n\n\n\n\ninput_code\ndescription\n\n\n\n\n0\nA\nAnnual\n\n\n1\nM\nMonthly\n\n\n2\nQ\nQuarterly\n\n\n\n\n\n\n\n\n\n\nNote that pandas data frames in Python can be a little difficult to work with, because Python doesn’t have a built-in variable explorer. If you’re doing data science, I recommend using an IDE like RStudio or Spyder that has a built-in variable explorer. However, if you don’t have a variable explorer, you can prevent Python from truncating data frames using the options in pandas. For instance, to increase the maximum allowed column width to 100 characters, we can use pandas.options.display.max_colwidth = 100.\nAlternatively, it’s possible to open the data frame in a new window to view it in full:\n\nimport imfp\nimport tempfile\nimport webbrowser\n\n# Define a simple function to view data frame in a browser window\ndef View(df):\n    html = df.to_html()\n    with tempfile.NamedTemporaryFile('w', delete=False, suffix='.html') as f:\n        url = 'file://' + f.name\n        f.write(html)\n    webbrowser.open(url)\n\n# Open data frame in a new browser window using the function\ndf = imfp.imf_databases()\nView(df)\n\n\n\n\nThere are two ways to supply parameters to imfp.imf_dataset: by supplying list arguments or by supplying a modified parameters dict. The list arguments workflow will be more intuitive for most users, but the dict argument workflow requires a little less code.\n\n\nTo supply list arguments, just find the codes you want and supply them to imfp.imf_dataset using the parameter name as the argument name. The example below shows how to request 2000–2015 annual coal prices from the Primary Commodity Price System database:\n\n# Fetch the 'freq' input code for annual frequency\nselected_freq = list(\n    params['freq']['input_code'][params['freq']['description'].str.contains(\"Annual\")]\n)\n\n# Fetch the 'commodity' input code for coal\nselected_commodity = list(\n    params['commodity']['input_code'][params['commodity']['description'].str.contains(\"Coal\")]\n)\n\n# Fetch the 'unit_measure' input code for index\nselected_unit_measure = list(\n    params['unit_measure']['input_code'][params['unit_measure']['description'].str.contains(\"Index\")]\n)\n\n# Request data from the API\ndf = imfp.imf_dataset(database_id = \"PCPS\",\n         freq = selected_freq, commodity = selected_commodity,\n         unit_measure = selected_unit_measure,\n         start_year = 2000, end_year = 2015)\n\n# Display the first few entries in the retrieved data frame\ndf.head()\n\n\n\n\n\n\n\n\nfreq\nref_area\ncommodity\nunit_measure\nunit_mult\ntime_format\ntime_period\nobs_value\n\n\n\n\n0\nA\nW00\nPCOAL\nIX\n0\nP1Y\n2000\n39.3510230293202\n\n\n1\nA\nW00\nPCOAL\nIX\n0\nP1Y\n2001\n49.3378587284039\n\n\n2\nA\nW00\nPCOAL\nIX\n0\nP1Y\n2002\n39.4949091648006\n\n\n3\nA\nW00\nPCOAL\nIX\n0\nP1Y\n2003\n43.2878876950788\n\n\n4\nA\nW00\nPCOAL\nIX\n0\nP1Y\n2004\n82.9185858052862\n\n\n\n\n\n\n\n\n\n\nTo supply a list object, modify each data frame in the params list object to retain only the rows you want, and then supply the modified list object to imfp.imf_dataset as its parameters argument. Here is how to make the same request for annual coal price data using a parameters list:\n\n# Fetch the 'freq' input code for annual frequency\nparams['freq'] = params['freq'][params['freq']['description'].str.contains(\"Annual\")]\n\n# Fetch the 'commodity' input code(s) for coal\nparams['commodity'] = params['commodity'][params['commodity']['description'].str.contains(\"Coal\")]\n\n# Fetch the 'unit_measure' input code for index\nparams['unit_measure'] = params['unit_measure'][params['unit_measure']['description'].str.contains(\"Index\")]\n\n# Request data from the API\ndf = imfp.imf_dataset(database_id = \"PCPS\",\n         parameters = params,\n         start_year = 2000, end_year = 2015)\n\n# Display the first few entries in the retrieved data frame\ndf.head()\n\n\n\n\n\n\n\n\nfreq\nref_area\ncommodity\nunit_measure\nunit_mult\ntime_format\ntime_period\nobs_value\n\n\n\n\n0\nA\nW00\nPCOAL\nIX\n0\nP1Y\n2000\n39.3510230293202\n\n\n1\nA\nW00\nPCOAL\nIX\n0\nP1Y\n2001\n49.3378587284039\n\n\n2\nA\nW00\nPCOAL\nIX\n0\nP1Y\n2002\n39.4949091648006\n\n\n3\nA\nW00\nPCOAL\nIX\n0\nP1Y\n2003\n43.2878876950788\n\n\n4\nA\nW00\nPCOAL\nIX\n0\nP1Y\n2004\n82.9185858052862\n\n\n\n\n\n\n\n\n\n\n\n\nNote that all columns in the returned data frame are character vectors, and that to plot the series we will need to convert to valid numeric or date formats. Using seaborn with hue, we can plot different indicators in different colors:\n\n# Convert obs_value to numeric and time_period to integer year\ndf = df.astype({\"time_period\" : int, \"obs_value\" : float})\n\n# Plot prices of different commodities in different colors with seaborn\nsns.lineplot(data=df, x='time_period', y='obs_value', hue='commodity');\n\n\n\n\n\n\n\n\nAlso note that the returned data frame has mysterious-looking codes as values in some columns.\nCodes in the time_format column are ISO 8601 duration codes. In this case, “P1Y” means “periods of 1 year.” The unit_mult column represents the number of zeroes you should add to the value column. For instance, if value is in millions, then the unit multiplier will be 6. If in billions, then the unit multiplier will be 9.\nThe meanings of the other codes are stored in our params object and can be fetched with a join. For instance to fetch the meaning of the ref_area code “W00”, we can perform a left join with the params['ref_area'] data frame and use select to replace ref_area with the parameter description:\n\n# Join df with params['ref_area'] to fetch code description\ndf = df.merge(params['ref_area'], left_on='ref_area',right_on='input_code',how='left')\n\n# Drop redundant columns and rename description column\ndf = df.drop(columns=['ref_area','input_code']).rename(columns={\"description\":\"ref_area\"})\n\n# View first few columns in the modified data frame\ndf.head()\n\n\n\n\n\n\n\n\nfreq\ncommodity\nunit_measure\nunit_mult\ntime_format\ntime_period\nobs_value\nref_area\n\n\n\n\n0\nA\nPCOAL\nIX\n0\nP1Y\n2000\n39.351023\nAll Countries, excluding the IO\n\n\n1\nA\nPCOAL\nIX\n0\nP1Y\n2001\n49.337859\nAll Countries, excluding the IO\n\n\n2\nA\nPCOAL\nIX\n0\nP1Y\n2002\n39.494909\nAll Countries, excluding the IO\n\n\n3\nA\nPCOAL\nIX\n0\nP1Y\n2003\n43.287888\nAll Countries, excluding the IO\n\n\n4\nA\nPCOAL\nIX\n0\nP1Y\n2004\n82.918586\nAll Countries, excluding the IO\n\n\n\n\n\n\n\n\n\n\n\n\nimfp.set_imf_app_name() allows users to set a custom application name to be used when making API calls to the IMF API. The IMF API has an application-based rate limit of 50 requests per second, with the application identified by the “user_agent” variable in the request header.\nThis could prove problematic if the imfp library became too popular and too many users tried to make simultaneous API requests using the default app name. By setting a custom application name, users can avoid hitting rate limits and being blocked by the API. imfp.set_imf_app_name() sets the application name by changing the IMF_APP_NAME variable in the environment. If this variable doesn’t exist, imfp.set_imf_app_name() will create it.\nTo set a custom application name, simply call the imfp.set_imf_app_name() function with your desired application name as an argument:\n\n# Set custom app name as an environment variable\nimfp.set_imf_app_name(\"my_custom_app_name\")\n\nThe function will throw an error if the provided name is missing, NULL, NA, not a string, or longer than 255 characters. If the provided name is “imfr” (the default) or an empty string, the function will issue a warning recommending the use of a unique app name to avoid hitting rate limits.\n\n\n\nBy default, imfp enforces a mandatory 1.5-second wait time between API calls to prevent repeated or recursive calls from exceeding the API’s bandwidth/rate limit. This wait time should be sufficient for most applications. However, if you are running parallel processes using imfp (e.g. during cross-platform testing), this wait time may be insufficient to prevent you from running up against the API’s rate and bandwidth limits. You can change this wait time by calling the set_imf_wait_time function with a numeric value, in seconds. For instance, to enforce a five-second wait time between API calls, use set_imf_wait_time(10).\nAlso note that by default, imfp functions will retry any API call rejected for bandwidth or rate limit reasons. The number of times imfp will attempt the call is set by the times argument, with a default value of 3. (With this value, requests will be retried twice after an initial failure.) Note that imfp enforces an exponentially increasing wait time between function calls, with a base wait time of 5 seconds on the first retry, so it is not recommended to set a high value for times.\n\n\n\n\n\nIf pyproject.toml version has been incremented, automatically deploy Github release from main with release notes auto-generated from News file or PR message\nImplement automatic build/render of readthedocs documentation with Sphinx\nRender/publish Github Pages documentation with Quarto\nAutomatically update all lockfile dependencies\nMove response mocking functionality from _download_parse to _imf_get\nInvestigate and implement different and more appropriate exception types, as we’re currently handling too many different cases with ValueError\nMore fully investigate the types of metadata available through the API and the most appropriate way to return them when a user calls include_metadata\nImplement optional response caching for imf_databases and imf_parameters\nSimplify and modularize some of the code, particularly in imf_dataset\n\n\n\n\nI would love to have your help in improving imfp. If you encounter a bug while using the library, please open an issue. Alternatively, fix the bug and open a pull request to the dev branch. Thanks in advance for your help!\nNote to maintainers: To deploy a new version of the package, increment the version number with poetry version patch (or minor/major for larger updates), update the lock file with the latest dependencies with poetry update, run the tests with pytest tests, make any necessary documentation changes in NEWS and README.ipynb, and push to dev. Github Actions will automatically format the code with black, render the README, and run the unit tests. If the workflow completes successfully, open a PR to main. After merging, the workflow will test again as one last sanity check and then automatically deploy the new version to PyPi. Currently, new Github releases must be created manually, but this will be automated in the future."
  },
  {
    "objectID": "index.html#installation",
    "href": "index.html#installation",
    "title": "imfp",
    "section": "",
    "text": "To install the stable version of imfp from PyPi, use pip.\npip install -q --upgrade imfp\nTo load the library, use import:\n\nimport imfp"
  },
  {
    "objectID": "index.html#usage",
    "href": "index.html#usage",
    "title": "imfp",
    "section": "",
    "text": "imfp outputs data in a pandas data frame, so you will want to use the pandas package for its functions for viewing and manipulating this object type. I also recommend matplotlib or seaborn for making plots, and numpy for computation. These packages can be installed using pip and loaded using import:\npip install -q pandas matplotlib seaborn numpy\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n\n\n\n\nThe imfp package introduces four core functions: imfp.imf_databases, imfp.imf_parameters, imfp.imf_parameter_defs, and imfp.imf_dataset. The function for downloading datasets is imfp.imf_dataset, but you will need the other functions to determine what arguments to supply to imfp.imf_dataset. For instance, all calls to imfp.imf_dataset require a database_id. This is because the IMF serves many different databases through its API, and the API needs to know which of these many databases you’re requesting data from. To obtain a list of databases, use imfp.imf_databases, like so:\n\n#Fetch the list of databases available through the IMF API\ndatabases = imfp.imf_databases()\ndatabases.head()\n\n\n\n\n\n\n\n\ndatabase_id\ndescription\n\n\n\n\n0\nBOP_2017M06\nBalance of Payments (BOP), 2017 M06\n\n\n1\nBOP_2020M3\nBalance of Payments (BOP), 2020 M03\n\n\n2\nBOP_2017M11\nBalance of Payments (BOP), 2017 M11\n\n\n3\nDOT_2020Q1\nDirection of Trade Statistics (DOTS), 2020 Q1\n\n\n4\nGFSMAB2016\nGovernment Finance Statistics Yearbook (GFSY 2...\n\n\n\n\n\n\n\nThis function returns the IMF’s listing of 259 databases available through the API. (In reality, 8 of the listed databases are defunct and not actually available: FAS_2015, GFS01, FM202010, APDREO202010, AFRREO202010, WHDREO202010, BOPAGG_2020, DOT_2020Q1.)\nTo view and explore the database list, it’s possible to explore subsets of the data frame by row number with databases.loc:\n\n# View a subset consisting of rows 5 through 9\ndatabases.loc[5:9]\n\n\n\n\n\n\n\n\ndatabase_id\ndescription\n\n\n\n\n5\nBOP_2019M12\nBalance of Payments (BOP), 2019 M12\n\n\n6\nGFSYFALCS2014\nGovernment Finance Statistics Yearbook (GFSY 2...\n\n\n7\nGFSE2016\nGovernment Finance Statistics Yearbook (GFSY 2...\n\n\n8\nFM201510\nFiscal Monitor (FM) October 2015\n\n\n9\nGFSIBS2016\nGovernment Finance Statistics Yearbook (GFSY 2...\n\n\n\n\n\n\n\nOr, if you already know which database you want, you can fetch the corresponding code by searching for a string match using str.contains and subsetting the data frame for matching rows. For instance, here’s how to search for commodities data:\n\ndatabases[databases['description'].str.contains(\"Commodity\")]\n\n\n\n\n\n\n\n\ndatabase_id\ndescription\n\n\n\n\n295\nPCTOT\nCommodity Terms of Trade\n\n\n302\nPCPS\nPrimary Commodity Price System (PCPS)\n\n\n\n\n\n\n\n\n\n\nOnce you have a database_id, it’s possible to make a call to imfp.imf_dataset to fetch the entire database: imfp.imf_dataset(database_id). However, while this will succeed for a few small databases, it will fail for all of the larger ones. And even in the rare case when it succeeds, fetching an entire database can take a long time. You’re much better off supplying additional filter parameters to reduce the size of your request.\nRequests to databases available through the IMF API are complicated by the fact that each database uses a different set of parameters when making a request. (At last count, there were 43 unique parameters used in making API requests from the various databases!) You also have to have the list of valid input codes for each parameter. The imfp.imf_parameters function solves this problem. Use the function to obtain the full list of parameters and valid input codes for a given database:\n\n# Fetch list of valid parameters and input codes for commodity price database\nparams = imfp.imf_parameters(\"PCPS\")\n\nThe imfp.imf_parameters function returns a dictionary of data frames. Each dictionary key name corresponds to a parameter used in making requests from the database:\n\n# Get key names from the params object\nparams.keys()\n\ndict_keys(['freq', 'ref_area', 'commodity', 'unit_measure'])\n\n\nIn the event that a parameter name is not self-explanatory, the imfp.imf_parameter_defs function can be used to fetch short text descriptions of each parameter:\n\n# Fetch and display parameter text descriptions for the commodity price database\nimfp.imf_parameter_defs(\"PCPS\")\n\n\n\n\n\n\n\n\nparameter\ndescription\n\n\n\n\n0\nfreq\nFrequency\n\n\n1\nref_area\nGeographical Areas\n\n\n2\ncommodity\nIndicator\n\n\n3\nunit_measure\nUnit\n\n\n\n\n\n\n\nEach named list item is a data frame containing a vector of valid input codes that can be used with the named parameter, and a vector of text descriptions of what each code represents.\nTo access the data frame containing valid values for each parameter, subset the params dict by the parameter name:\n\n# View the data frame of valid input codes for the frequency parameter\nparams['freq']\n\n\n\n\n\n\n\n\ninput_code\ndescription\n\n\n\n\n0\nA\nAnnual\n\n\n1\nM\nMonthly\n\n\n2\nQ\nQuarterly\n\n\n\n\n\n\n\n\n\n\nNote that pandas data frames in Python can be a little difficult to work with, because Python doesn’t have a built-in variable explorer. If you’re doing data science, I recommend using an IDE like RStudio or Spyder that has a built-in variable explorer. However, if you don’t have a variable explorer, you can prevent Python from truncating data frames using the options in pandas. For instance, to increase the maximum allowed column width to 100 characters, we can use pandas.options.display.max_colwidth = 100.\nAlternatively, it’s possible to open the data frame in a new window to view it in full:\n\nimport imfp\nimport tempfile\nimport webbrowser\n\n# Define a simple function to view data frame in a browser window\ndef View(df):\n    html = df.to_html()\n    with tempfile.NamedTemporaryFile('w', delete=False, suffix='.html') as f:\n        url = 'file://' + f.name\n        f.write(html)\n    webbrowser.open(url)\n\n# Open data frame in a new browser window using the function\ndf = imfp.imf_databases()\nView(df)\n\n\n\n\nThere are two ways to supply parameters to imfp.imf_dataset: by supplying list arguments or by supplying a modified parameters dict. The list arguments workflow will be more intuitive for most users, but the dict argument workflow requires a little less code.\n\n\nTo supply list arguments, just find the codes you want and supply them to imfp.imf_dataset using the parameter name as the argument name. The example below shows how to request 2000–2015 annual coal prices from the Primary Commodity Price System database:\n\n# Fetch the 'freq' input code for annual frequency\nselected_freq = list(\n    params['freq']['input_code'][params['freq']['description'].str.contains(\"Annual\")]\n)\n\n# Fetch the 'commodity' input code for coal\nselected_commodity = list(\n    params['commodity']['input_code'][params['commodity']['description'].str.contains(\"Coal\")]\n)\n\n# Fetch the 'unit_measure' input code for index\nselected_unit_measure = list(\n    params['unit_measure']['input_code'][params['unit_measure']['description'].str.contains(\"Index\")]\n)\n\n# Request data from the API\ndf = imfp.imf_dataset(database_id = \"PCPS\",\n         freq = selected_freq, commodity = selected_commodity,\n         unit_measure = selected_unit_measure,\n         start_year = 2000, end_year = 2015)\n\n# Display the first few entries in the retrieved data frame\ndf.head()\n\n\n\n\n\n\n\n\nfreq\nref_area\ncommodity\nunit_measure\nunit_mult\ntime_format\ntime_period\nobs_value\n\n\n\n\n0\nA\nW00\nPCOAL\nIX\n0\nP1Y\n2000\n39.3510230293202\n\n\n1\nA\nW00\nPCOAL\nIX\n0\nP1Y\n2001\n49.3378587284039\n\n\n2\nA\nW00\nPCOAL\nIX\n0\nP1Y\n2002\n39.4949091648006\n\n\n3\nA\nW00\nPCOAL\nIX\n0\nP1Y\n2003\n43.2878876950788\n\n\n4\nA\nW00\nPCOAL\nIX\n0\nP1Y\n2004\n82.9185858052862\n\n\n\n\n\n\n\n\n\n\nTo supply a list object, modify each data frame in the params list object to retain only the rows you want, and then supply the modified list object to imfp.imf_dataset as its parameters argument. Here is how to make the same request for annual coal price data using a parameters list:\n\n# Fetch the 'freq' input code for annual frequency\nparams['freq'] = params['freq'][params['freq']['description'].str.contains(\"Annual\")]\n\n# Fetch the 'commodity' input code(s) for coal\nparams['commodity'] = params['commodity'][params['commodity']['description'].str.contains(\"Coal\")]\n\n# Fetch the 'unit_measure' input code for index\nparams['unit_measure'] = params['unit_measure'][params['unit_measure']['description'].str.contains(\"Index\")]\n\n# Request data from the API\ndf = imfp.imf_dataset(database_id = \"PCPS\",\n         parameters = params,\n         start_year = 2000, end_year = 2015)\n\n# Display the first few entries in the retrieved data frame\ndf.head()\n\n\n\n\n\n\n\n\nfreq\nref_area\ncommodity\nunit_measure\nunit_mult\ntime_format\ntime_period\nobs_value\n\n\n\n\n0\nA\nW00\nPCOAL\nIX\n0\nP1Y\n2000\n39.3510230293202\n\n\n1\nA\nW00\nPCOAL\nIX\n0\nP1Y\n2001\n49.3378587284039\n\n\n2\nA\nW00\nPCOAL\nIX\n0\nP1Y\n2002\n39.4949091648006\n\n\n3\nA\nW00\nPCOAL\nIX\n0\nP1Y\n2003\n43.2878876950788\n\n\n4\nA\nW00\nPCOAL\nIX\n0\nP1Y\n2004\n82.9185858052862"
  },
  {
    "objectID": "index.html#working-with-the-returned-data-frame",
    "href": "index.html#working-with-the-returned-data-frame",
    "title": "imfp",
    "section": "",
    "text": "Note that all columns in the returned data frame are character vectors, and that to plot the series we will need to convert to valid numeric or date formats. Using seaborn with hue, we can plot different indicators in different colors:\n\n# Convert obs_value to numeric and time_period to integer year\ndf = df.astype({\"time_period\" : int, \"obs_value\" : float})\n\n# Plot prices of different commodities in different colors with seaborn\nsns.lineplot(data=df, x='time_period', y='obs_value', hue='commodity');\n\n\n\n\n\n\n\n\nAlso note that the returned data frame has mysterious-looking codes as values in some columns.\nCodes in the time_format column are ISO 8601 duration codes. In this case, “P1Y” means “periods of 1 year.” The unit_mult column represents the number of zeroes you should add to the value column. For instance, if value is in millions, then the unit multiplier will be 6. If in billions, then the unit multiplier will be 9.\nThe meanings of the other codes are stored in our params object and can be fetched with a join. For instance to fetch the meaning of the ref_area code “W00”, we can perform a left join with the params['ref_area'] data frame and use select to replace ref_area with the parameter description:\n\n# Join df with params['ref_area'] to fetch code description\ndf = df.merge(params['ref_area'], left_on='ref_area',right_on='input_code',how='left')\n\n# Drop redundant columns and rename description column\ndf = df.drop(columns=['ref_area','input_code']).rename(columns={\"description\":\"ref_area\"})\n\n# View first few columns in the modified data frame\ndf.head()\n\n\n\n\n\n\n\n\nfreq\ncommodity\nunit_measure\nunit_mult\ntime_format\ntime_period\nobs_value\nref_area\n\n\n\n\n0\nA\nPCOAL\nIX\n0\nP1Y\n2000\n39.351023\nAll Countries, excluding the IO\n\n\n1\nA\nPCOAL\nIX\n0\nP1Y\n2001\n49.337859\nAll Countries, excluding the IO\n\n\n2\nA\nPCOAL\nIX\n0\nP1Y\n2002\n39.494909\nAll Countries, excluding the IO\n\n\n3\nA\nPCOAL\nIX\n0\nP1Y\n2003\n43.287888\nAll Countries, excluding the IO\n\n\n4\nA\nPCOAL\nIX\n0\nP1Y\n2004\n82.918586\nAll Countries, excluding the IO"
  },
  {
    "objectID": "index.html#rate-and-bandwidth-limit-management",
    "href": "index.html#rate-and-bandwidth-limit-management",
    "title": "imfp",
    "section": "",
    "text": "imfp.set_imf_app_name() allows users to set a custom application name to be used when making API calls to the IMF API. The IMF API has an application-based rate limit of 50 requests per second, with the application identified by the “user_agent” variable in the request header.\nThis could prove problematic if the imfp library became too popular and too many users tried to make simultaneous API requests using the default app name. By setting a custom application name, users can avoid hitting rate limits and being blocked by the API. imfp.set_imf_app_name() sets the application name by changing the IMF_APP_NAME variable in the environment. If this variable doesn’t exist, imfp.set_imf_app_name() will create it.\nTo set a custom application name, simply call the imfp.set_imf_app_name() function with your desired application name as an argument:\n\n# Set custom app name as an environment variable\nimfp.set_imf_app_name(\"my_custom_app_name\")\n\nThe function will throw an error if the provided name is missing, NULL, NA, not a string, or longer than 255 characters. If the provided name is “imfr” (the default) or an empty string, the function will issue a warning recommending the use of a unique app name to avoid hitting rate limits.\n\n\n\nBy default, imfp enforces a mandatory 1.5-second wait time between API calls to prevent repeated or recursive calls from exceeding the API’s bandwidth/rate limit. This wait time should be sufficient for most applications. However, if you are running parallel processes using imfp (e.g. during cross-platform testing), this wait time may be insufficient to prevent you from running up against the API’s rate and bandwidth limits. You can change this wait time by calling the set_imf_wait_time function with a numeric value, in seconds. For instance, to enforce a five-second wait time between API calls, use set_imf_wait_time(10).\nAlso note that by default, imfp functions will retry any API call rejected for bandwidth or rate limit reasons. The number of times imfp will attempt the call is set by the times argument, with a default value of 3. (With this value, requests will be retried twice after an initial failure.) Note that imfp enforces an exponentially increasing wait time between function calls, with a base wait time of 5 seconds on the first retry, so it is not recommended to set a high value for times."
  },
  {
    "objectID": "index.html#planned-features",
    "href": "index.html#planned-features",
    "title": "imfp",
    "section": "",
    "text": "If pyproject.toml version has been incremented, automatically deploy Github release from main with release notes auto-generated from News file or PR message\nImplement automatic build/render of readthedocs documentation with Sphinx\nRender/publish Github Pages documentation with Quarto\nAutomatically update all lockfile dependencies\nMove response mocking functionality from _download_parse to _imf_get\nInvestigate and implement different and more appropriate exception types, as we’re currently handling too many different cases with ValueError\nMore fully investigate the types of metadata available through the API and the most appropriate way to return them when a user calls include_metadata\nImplement optional response caching for imf_databases and imf_parameters\nSimplify and modularize some of the code, particularly in imf_dataset"
  },
  {
    "objectID": "index.html#contributing",
    "href": "index.html#contributing",
    "title": "imfp",
    "section": "",
    "text": "I would love to have your help in improving imfp. If you encounter a bug while using the library, please open an issue. Alternatively, fix the bug and open a pull request to the dev branch. Thanks in advance for your help!\nNote to maintainers: To deploy a new version of the package, increment the version number with poetry version patch (or minor/major for larger updates), update the lock file with the latest dependencies with poetry update, run the tests with pytest tests, make any necessary documentation changes in NEWS and README.ipynb, and push to dev. Github Actions will automatically format the code with black, render the README, and run the unit tests. If the workflow completes successfully, open a PR to main. After merging, the workflow will test again as one last sanity check and then automatically deploy the new version to PyPi. Currently, new Github releases must be created manually, but this will be automated in the future."
  }
]