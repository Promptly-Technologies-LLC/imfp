[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "imfp",
    "section": "",
    "text": "imfp, created and maintained by Promptly Technologies, is a Python package for downloading data from the International Monetary Fund’s RESTful JSON API.\nNOTE: The IMF recently discontinued their SDMX 2.0 API, which imfp used, and migrated to 3.0. Version 1.3.0 of imfp represents my best effort to make a semi-backward compatible version of the library. Function names remain the same, and I have added a few helpers to map old parameters to the new ones. But the honest truth is that the structure of the API and the data returned by it has changed very significantly, and your old code is very unlikely to work without some modifications. I apologize for the inconvenience. imfp version 2.0.0 will be released soon, and will rewrite the library more comprehensively to follow the patterns of the new API.\n\n\nTo install the stable version of imfp from PyPi, use pip.\npip install --upgrade imfp\nTo load the library, use import:\n\nimport imfp\n\n\n\n\nThe imfp package introduces four core functions: imf_databases, imf_parameters, imf_parameter_defs, and imf_dataset. The function for downloading datasets is imf_dataset, but you will need the other functions to determine what arguments to supply to your imf_dataset function call.\n\n\nFor instance, all calls to imf_dataset require a database_id. This is because the IMF serves many different databases through its API, and the API needs to know which of these many databases you’re requesting data from.\nTo fetch a list of available databases, use:\n\n# Fetch list of available databases\ndatabases = imfp.imf_databases()\n\nSee Working with Databases for more information.\n\n\n\nRequests to fetch data from IMF databases are complicated by the fact that each database uses a different set of parameters when making a request. (At last count, there were 43 unique parameters used in making API requests from the various databases!) You also have to have the list of valid input codes for each parameter. See Working with Parameters for a more detailed explanation of parameters and input codes and how they work.\nTo obtain the full list of parameters and valid input codes for a given database, use:\n\n# Fetch list of valid parameters and input codes for commodity price database\nparams = imfp.imf_parameters(\"PCPS\")\n\nThe imf_parameters function returns a dictionary of data frames. Each dictionary key name corresponds to a parameter used in making requests from the database:\n\n# Get key names from the params object\nparams.keys()\n\ndict_keys(['country', 'indicator', 'data_transformation', 'frequency'])\n\n\nEach named list item is a data frame containing the valid input codes (and their descriptions) that can be used with the named parameter.\nTo access the data frame containing valid values for each parameter, subset the params dict by the parameter name:\n\n# View the data frame of valid input codes for the frequency parameter\nparams['frequency']\n\n\n\n\n\n\n\n\ninput_code\ndescription\n\n\n\n\n0\nA\nAnnual\n\n\n1\nD\nDaily\n\n\n2\nM\nMonthly\n\n\n3\nQ\nQuarterly\n\n\n4\nS\nHalf-yearly, semester\n\n\n5\nW\nWeekly\n\n\n\n\n\n\n\n\n\n\nTo make a request to fetch data from the IMF API, just call imfp.imf_dataset with the database ID and keyword arguments for each parameter, where the keyword argument name is the parameter name and the value is the list of codes you want.\nFor instance, on exploring the frequency parameter of the Primary Commodity Price System database above, we found that the frequency can take one of three values: “A” for annual, “Q” for quarterly, and “M” for monthly. Thus, to request annual data, we can call imfp.imf_dataset with frequency = [\"A\"].\nSimilarly, we might search the dataframes of valid input codes for the indicator and data_transformation parameters to find the input codes for coal and index:\n\n# Find the 'indicator' input code for coal\nparams['indicator'].loc[\n    params['indicator']['description'].str.contains(\"Coal\")\n]\n\n\n\n\n\n\n\n\ninput_code\ndescription\n\n\n\n\n14\nPCOAL\nCoal index, Commodity price index, Index, 2016...\n\n\n15\nPCOALAU\nCoal, Australia, US dollars per metric tonne, ...\n\n\n16\nPCOALSA\nCoal, South Africa, US dollars per metric tonn...\n\n\n\n\n\n\n\n\n# Find the 'data_transformation' input code for index\nparams['data_transformation'].loc[\n    params['data_transformation']['description'].str.contains(\"Index\")\n]\n\n\n\n\n\n\n\n\ninput_code\ndescription\n\n\n\n\n0\nINDEX\nIndex\n\n\n1\nINDEX_PCH\nIndex, percent change\n\n\n2\nINDEX_PCHY\nIndex, percent change from a year ago\n\n\n\n\n\n\n\nFinally, we can use the information we’ve gathered to make the request to fetch the data:\n\n# Request data from the API\ndf = imfp.imf_dataset(database_id = \"PCPS\",\n         frequency = [\"A\"], indicator = [\"PCOAL\"],\n         data_transformation = [\"INDEX\"])\n\n# Display the first few entries in the retrieved data frame\ndf.head()\n\n\n\n\n\n\n\n\ncountry\nindicator\ndata_transformation\nfrequency\ntime_period\nobs_value\n\n\n\n\n0\nG001\nPCOAL\nINDEX\nA\n1992\n49.892138\n\n\n1\nG001\nPCOAL\nINDEX\nA\n1993\n43.279151\n\n\n2\nG001\nPCOAL\nINDEX\nA\n1994\n45.213931\n\n\n3\nG001\nPCOAL\nINDEX\nA\n1995\n55.433711\n\n\n4\nG001\nPCOAL\nINDEX\nA\n1996\n53.179458\n\n\n\n\n\n\n\n\n\n\n\nNote that all columns in the returned data frame are string objects, and to plot the series we will need to convert to valid numeric or date formats:\n\n# Convert obs_value to numeric and time_period to integer year\ndf = df.astype({\"time_period\" : int, \"obs_value\" : float})\n\nThen, using seaborn with hue, we can plot different indicators in different colors:\n\nimport seaborn as sns\n\n# Plot prices of different commodities in different colors with seaborn\nsns.lineplot(data=df, x='time_period', y='obs_value', hue='indicator');\n\n/home/runner/work/imfp/imfp/.venv/lib/python3.14/site-packages/seaborn/relational.py:300: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\nYou are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\nA typical example is when you are setting values in a column of a DataFrame, like:\n\ndf[\"col\"][row_indexer] = value\n\nUse `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n  sub_data[f\"{other}min\"] = np.nan\n/home/runner/work/imfp/imfp/.venv/lib/python3.14/site-packages/seaborn/relational.py:301: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\nYou are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\nA typical example is when you are setting values in a column of a DataFrame, like:\n\ndf[\"col\"][row_indexer] = value\n\nUse `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n  sub_data[f\"{other}max\"] = np.nan\n/home/runner/work/imfp/imfp/.venv/lib/python3.14/site-packages/seaborn/relational.py:307: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\nYou are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\nA typical example is when you are setting values in a column of a DataFrame, like:\n\ndf[\"col\"][row_indexer] = value\n\nUse `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n  sub_data[col] = inv(sub_data[col])\n\n\n\n\n\n\n\n\n\n\n\n\nWe welcome contributions to improve imfp! Here’s how you can help:\n\nIf you find a bug, please open a Github issue\nTo fix a bug:\n\nFork and clone the repository and open a terminal in the repository directory\nInstall uv with curl -LsSf https://astral.sh/uv/install.sh | sh\nInstall the dependencies with uv sync\nInstall a git hook to enforce conventional commits with curl -o- https://raw.githubusercontent.com/tapsellorg/conventional-commits-git-hook/master/scripts/install.sh | sh\nCreate a fix, commit it with an “Angular-style Conventional Commit” message, and push it to your fork\nOpen a pull request to our main branch\n\n\nNote that if you want to change and preview the documentation, you will need to install the Quarto CLI tool.\nVersion incrementing, package building, testing, changelog generation, documentation rendering, publishing to PyPI, and Github release creation is handled automatically by the GitHub Actions workflow based on the commit messages."
  },
  {
    "objectID": "index.html#installation",
    "href": "index.html#installation",
    "title": "imfp",
    "section": "",
    "text": "To install the stable version of imfp from PyPi, use pip.\npip install --upgrade imfp\nTo load the library, use import:\n\nimport imfp"
  },
  {
    "objectID": "index.html#workflow",
    "href": "index.html#workflow",
    "title": "imfp",
    "section": "",
    "text": "The imfp package introduces four core functions: imf_databases, imf_parameters, imf_parameter_defs, and imf_dataset. The function for downloading datasets is imf_dataset, but you will need the other functions to determine what arguments to supply to your imf_dataset function call.\n\n\nFor instance, all calls to imf_dataset require a database_id. This is because the IMF serves many different databases through its API, and the API needs to know which of these many databases you’re requesting data from.\nTo fetch a list of available databases, use:\n\n# Fetch list of available databases\ndatabases = imfp.imf_databases()\n\nSee Working with Databases for more information.\n\n\n\nRequests to fetch data from IMF databases are complicated by the fact that each database uses a different set of parameters when making a request. (At last count, there were 43 unique parameters used in making API requests from the various databases!) You also have to have the list of valid input codes for each parameter. See Working with Parameters for a more detailed explanation of parameters and input codes and how they work.\nTo obtain the full list of parameters and valid input codes for a given database, use:\n\n# Fetch list of valid parameters and input codes for commodity price database\nparams = imfp.imf_parameters(\"PCPS\")\n\nThe imf_parameters function returns a dictionary of data frames. Each dictionary key name corresponds to a parameter used in making requests from the database:\n\n# Get key names from the params object\nparams.keys()\n\ndict_keys(['country', 'indicator', 'data_transformation', 'frequency'])\n\n\nEach named list item is a data frame containing the valid input codes (and their descriptions) that can be used with the named parameter.\nTo access the data frame containing valid values for each parameter, subset the params dict by the parameter name:\n\n# View the data frame of valid input codes for the frequency parameter\nparams['frequency']\n\n\n\n\n\n\n\n\ninput_code\ndescription\n\n\n\n\n0\nA\nAnnual\n\n\n1\nD\nDaily\n\n\n2\nM\nMonthly\n\n\n3\nQ\nQuarterly\n\n\n4\nS\nHalf-yearly, semester\n\n\n5\nW\nWeekly\n\n\n\n\n\n\n\n\n\n\nTo make a request to fetch data from the IMF API, just call imfp.imf_dataset with the database ID and keyword arguments for each parameter, where the keyword argument name is the parameter name and the value is the list of codes you want.\nFor instance, on exploring the frequency parameter of the Primary Commodity Price System database above, we found that the frequency can take one of three values: “A” for annual, “Q” for quarterly, and “M” for monthly. Thus, to request annual data, we can call imfp.imf_dataset with frequency = [\"A\"].\nSimilarly, we might search the dataframes of valid input codes for the indicator and data_transformation parameters to find the input codes for coal and index:\n\n# Find the 'indicator' input code for coal\nparams['indicator'].loc[\n    params['indicator']['description'].str.contains(\"Coal\")\n]\n\n\n\n\n\n\n\n\ninput_code\ndescription\n\n\n\n\n14\nPCOAL\nCoal index, Commodity price index, Index, 2016...\n\n\n15\nPCOALAU\nCoal, Australia, US dollars per metric tonne, ...\n\n\n16\nPCOALSA\nCoal, South Africa, US dollars per metric tonn...\n\n\n\n\n\n\n\n\n# Find the 'data_transformation' input code for index\nparams['data_transformation'].loc[\n    params['data_transformation']['description'].str.contains(\"Index\")\n]\n\n\n\n\n\n\n\n\ninput_code\ndescription\n\n\n\n\n0\nINDEX\nIndex\n\n\n1\nINDEX_PCH\nIndex, percent change\n\n\n2\nINDEX_PCHY\nIndex, percent change from a year ago\n\n\n\n\n\n\n\nFinally, we can use the information we’ve gathered to make the request to fetch the data:\n\n# Request data from the API\ndf = imfp.imf_dataset(database_id = \"PCPS\",\n         frequency = [\"A\"], indicator = [\"PCOAL\"],\n         data_transformation = [\"INDEX\"])\n\n# Display the first few entries in the retrieved data frame\ndf.head()\n\n\n\n\n\n\n\n\ncountry\nindicator\ndata_transformation\nfrequency\ntime_period\nobs_value\n\n\n\n\n0\nG001\nPCOAL\nINDEX\nA\n1992\n49.892138\n\n\n1\nG001\nPCOAL\nINDEX\nA\n1993\n43.279151\n\n\n2\nG001\nPCOAL\nINDEX\nA\n1994\n45.213931\n\n\n3\nG001\nPCOAL\nINDEX\nA\n1995\n55.433711\n\n\n4\nG001\nPCOAL\nINDEX\nA\n1996\n53.179458"
  },
  {
    "objectID": "index.html#working-with-the-returned-data-frame",
    "href": "index.html#working-with-the-returned-data-frame",
    "title": "imfp",
    "section": "",
    "text": "Note that all columns in the returned data frame are string objects, and to plot the series we will need to convert to valid numeric or date formats:\n\n# Convert obs_value to numeric and time_period to integer year\ndf = df.astype({\"time_period\" : int, \"obs_value\" : float})\n\nThen, using seaborn with hue, we can plot different indicators in different colors:\n\nimport seaborn as sns\n\n# Plot prices of different commodities in different colors with seaborn\nsns.lineplot(data=df, x='time_period', y='obs_value', hue='indicator');\n\n/home/runner/work/imfp/imfp/.venv/lib/python3.14/site-packages/seaborn/relational.py:300: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\nYou are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\nA typical example is when you are setting values in a column of a DataFrame, like:\n\ndf[\"col\"][row_indexer] = value\n\nUse `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n  sub_data[f\"{other}min\"] = np.nan\n/home/runner/work/imfp/imfp/.venv/lib/python3.14/site-packages/seaborn/relational.py:301: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\nYou are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\nA typical example is when you are setting values in a column of a DataFrame, like:\n\ndf[\"col\"][row_indexer] = value\n\nUse `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n  sub_data[f\"{other}max\"] = np.nan\n/home/runner/work/imfp/imfp/.venv/lib/python3.14/site-packages/seaborn/relational.py:307: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\nYou are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\nA typical example is when you are setting values in a column of a DataFrame, like:\n\ndf[\"col\"][row_indexer] = value\n\nUse `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n  sub_data[col] = inv(sub_data[col])"
  },
  {
    "objectID": "index.html#contributing",
    "href": "index.html#contributing",
    "title": "imfp",
    "section": "",
    "text": "We welcome contributions to improve imfp! Here’s how you can help:\n\nIf you find a bug, please open a Github issue\nTo fix a bug:\n\nFork and clone the repository and open a terminal in the repository directory\nInstall uv with curl -LsSf https://astral.sh/uv/install.sh | sh\nInstall the dependencies with uv sync\nInstall a git hook to enforce conventional commits with curl -o- https://raw.githubusercontent.com/tapsellorg/conventional-commits-git-hook/master/scripts/install.sh | sh\nCreate a fix, commit it with an “Angular-style Conventional Commit” message, and push it to your fork\nOpen a pull request to our main branch\n\n\nNote that if you want to change and preview the documentation, you will need to install the Quarto CLI tool.\nVersion incrementing, package building, testing, changelog generation, documentation rendering, publishing to PyPI, and Github release creation is handled automatically by the GitHub Actions workflow based on the commit messages."
  },
  {
    "objectID": "docs/rate_limits.html",
    "href": "docs/rate_limits.html",
    "title": "Rate Limits",
    "section": "",
    "text": "The IMF API imposes very restrictive (and incompletely documented) rate limits, not only for individual users and applications, but also globally for all users of the API. Thus, at high-traffic times, you may find that your requests fail. It’s highly recommended that you set up proactive error handling, wait times, retries, and request caching to avoid hitting the API’s rate limits. The imfp library provides some tools to help you do this (with more planned for future releases).\n\n\nThe IMF API has an application-based rate limit of 50 requests per second. Each application is identified by the “user_agent” variable in the request header. By default, all imfp users share the same application name, which could lead to rate limit issues if many users are making requests simultaneously.\nThis could prove problematic if the imfp library became too popular and too many users tried to make simultaneous API requests using the default app name. By setting a custom application name, users can avoid hitting this rate limit and being blocked by the API. To solve this problem, imfp supplies the set_imf_app_name() function to set a custom application name.\nset_imf_app_name() sets the application name by changing the IMF_APP_NAME variable in the environment. If this variable doesn’t exist, set_imf_app_name() will create it. To set a custom application name, simply call the set_imf_app_name() function with your desired application name as an argument:\n\nimport imfp\n\n# Set custom app name as an environment variable\nimfp.set_imf_app_name(\"my_custom_app_name\")\n\nThe function will throw an error if the provided name is missing, NULL, NA, not a string, or longer than 255 characters. If the provided name is “imfp” (the default) or an empty string, the function will issue a warning recommending the use of a unique app name to avoid hitting rate limits.\n\n\n\nIf making multiple requests in a short period of time, you may want to increase the wait time between requests to avoid hitting the API’s rate limits. This is done with the set_imf_wait_time() function:\n\n# Increase wait time to 5 seconds\nimfp.set_imf_wait_time(5)\n\n\n\n\nimfp automatically handles rate limits with exponential backoff:\n\nWaits for specified time\nRetries the request\nIncreases wait time exponentially on subsequent failures\nStops after 3 attempts (default)\n\nYou can modify retry behavior:\n\n# Retry 4 times rather than the default 3\ndf = imfp.imf_dataset(\"PPI\", \"WPI\", times=4)\n\n\n\n\nTo reduce API calls, you can cache frequently accessed data. For instance, in a Jupyter or Quarto notebook that you run multiple times, you can wrap each imfp function call in an if statement that checks if the returned data has already been saved to a file. If it has, it loads the data from the file. If it hasn’t, it fetches the data from the API and saves it to a file.\nNote that to run this code, you will need to install the pyarrow library, which pandas uses as its engine for reading and writing parquet files (but which is not installed with pandas or imfp by default). Use pip install pyarrow to install it.\n\nimport os\nimport pandas as pd\n\n# Fetch imf databases from file if available, else from API\ncache_path = f\"data/imf_databases.parquet\"\nif os.path.exists(cache_path):\n    databases = pd.read_parquet(cache_path)\nelse:\n    databases = imfp.imf_databases()\n    os.makedirs(\"data\", exist_ok=True)\n    databases.to_parquet(cache_path)\n\nYou can also functionalize this logic to permit reuse several times in the same script or notebook. See Jenny Xu’s excellent demo notebook for example caching functions."
  },
  {
    "objectID": "docs/rate_limits.html#api-rate-management",
    "href": "docs/rate_limits.html#api-rate-management",
    "title": "Rate Limits",
    "section": "",
    "text": "The IMF API imposes very restrictive (and incompletely documented) rate limits, not only for individual users and applications, but also globally for all users of the API. Thus, at high-traffic times, you may find that your requests fail. It’s highly recommended that you set up proactive error handling, wait times, retries, and request caching to avoid hitting the API’s rate limits. The imfp library provides some tools to help you do this (with more planned for future releases).\n\n\nThe IMF API has an application-based rate limit of 50 requests per second. Each application is identified by the “user_agent” variable in the request header. By default, all imfp users share the same application name, which could lead to rate limit issues if many users are making requests simultaneously.\nThis could prove problematic if the imfp library became too popular and too many users tried to make simultaneous API requests using the default app name. By setting a custom application name, users can avoid hitting this rate limit and being blocked by the API. To solve this problem, imfp supplies the set_imf_app_name() function to set a custom application name.\nset_imf_app_name() sets the application name by changing the IMF_APP_NAME variable in the environment. If this variable doesn’t exist, set_imf_app_name() will create it. To set a custom application name, simply call the set_imf_app_name() function with your desired application name as an argument:\n\nimport imfp\n\n# Set custom app name as an environment variable\nimfp.set_imf_app_name(\"my_custom_app_name\")\n\nThe function will throw an error if the provided name is missing, NULL, NA, not a string, or longer than 255 characters. If the provided name is “imfp” (the default) or an empty string, the function will issue a warning recommending the use of a unique app name to avoid hitting rate limits.\n\n\n\nIf making multiple requests in a short period of time, you may want to increase the wait time between requests to avoid hitting the API’s rate limits. This is done with the set_imf_wait_time() function:\n\n# Increase wait time to 5 seconds\nimfp.set_imf_wait_time(5)\n\n\n\n\nimfp automatically handles rate limits with exponential backoff:\n\nWaits for specified time\nRetries the request\nIncreases wait time exponentially on subsequent failures\nStops after 3 attempts (default)\n\nYou can modify retry behavior:\n\n# Retry 4 times rather than the default 3\ndf = imfp.imf_dataset(\"PPI\", \"WPI\", times=4)\n\n\n\n\nTo reduce API calls, you can cache frequently accessed data. For instance, in a Jupyter or Quarto notebook that you run multiple times, you can wrap each imfp function call in an if statement that checks if the returned data has already been saved to a file. If it has, it loads the data from the file. If it hasn’t, it fetches the data from the API and saves it to a file.\nNote that to run this code, you will need to install the pyarrow library, which pandas uses as its engine for reading and writing parquet files (but which is not installed with pandas or imfp by default). Use pip install pyarrow to install it.\n\nimport os\nimport pandas as pd\n\n# Fetch imf databases from file if available, else from API\ncache_path = f\"data/imf_databases.parquet\"\nif os.path.exists(cache_path):\n    databases = pd.read_parquet(cache_path)\nelse:\n    databases = imfp.imf_databases()\n    os.makedirs(\"data\", exist_ok=True)\n    databases.to_parquet(cache_path)\n\nYou can also functionalize this logic to permit reuse several times in the same script or notebook. See Jenny Xu’s excellent demo notebook for example caching functions."
  },
  {
    "objectID": "docs/rate_limits.html#performance-tips",
    "href": "docs/rate_limits.html#performance-tips",
    "title": "Rate Limits",
    "section": "Performance Tips",
    "text": "Performance Tips\n\nFilter Early: Use parameters to limit data at the API level\nParallelize Carefully: Avoid running parallel API requests, even from multiple clients\nUse Efficient Formats: Store cached data in parquet or feather files\nValidate Data: Check for errors and empty responses"
  },
  {
    "objectID": "docs/installation.html",
    "href": "docs/installation.html",
    "title": "Installation",
    "section": "",
    "text": "To install the latest version of imfp, you will need to have Python 3.10 or later installed on your system.\nIf you don’t already have Python, we recommend installing the uv package manager and installing Python with uv python install."
  },
  {
    "objectID": "docs/installation.html#prerequisites",
    "href": "docs/installation.html#prerequisites",
    "title": "Installation",
    "section": "",
    "text": "To install the latest version of imfp, you will need to have Python 3.10 or later installed on your system.\nIf you don’t already have Python, we recommend installing the uv package manager and installing Python with uv python install."
  },
  {
    "objectID": "docs/installation.html#installation",
    "href": "docs/installation.html#installation",
    "title": "Installation",
    "section": "Installation",
    "text": "Installation\nTo install the latest stable imfp wheel from PyPi using pip:\npip install --upgrade imfp\nAlternatively, to install from the source code on Github, you can use the following command:\npip install --upgrade git+https://github.com/Promptly-Technologies-LLC/imfp.git\nYou can then import the package in your Python script:\nimport imfp"
  },
  {
    "objectID": "docs/installation.html#suggested-dependencies-for-data-analysis",
    "href": "docs/installation.html#suggested-dependencies-for-data-analysis",
    "title": "Installation",
    "section": "Suggested Dependencies for Data Analysis",
    "text": "Suggested Dependencies for Data Analysis\nimfp outputs data in a pandas data frame, so you will want to use the pandas package (which is installed with imfp) for its functions for viewing and manipulating this object type. For data visualization, we recommend installing these additional packages:\npip install -q matplotlib seaborn\nYou can then import these packages in your Python script:\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns"
  },
  {
    "objectID": "docs/installation.html#development-installation",
    "href": "docs/installation.html#development-installation",
    "title": "Installation",
    "section": "Development Installation",
    "text": "Development Installation\nTo get started with development of imfp,\n\nFork and clone the repository\nInstall uv with curl -LsSf https://astral.sh/uv/install.sh | sh\nInstall the dependencies with uv sync\nInstall a git pre-commit hook to enforce conventional commits:\ncurl -o- https://raw.githubusercontent.com/tapsellorg/conventional-commits-git-hook/master/scripts/install.sh | sh\n\nTo edit and preview the documentation, you’ll also want to install the Quarto CLI tool."
  },
  {
    "objectID": "docs/datasets.html",
    "href": "docs/datasets.html",
    "title": "Requesting Datasets",
    "section": "",
    "text": "To retrieve data from an IMF database, you’ll need the database ID and any relevant filter parameters. Here’s a basic example using the Producer Price Index (PPI) database from 2000 to 2015:\n\nimport imfp\n\n# Get parameters and their valid codes\nparams = imfp.imf_parameters(\"PPI\")\n\n# Fetch annual coal price index data\ndf = imfp.imf_dataset(\n    database_id=\"PPI\",\n    frequency=[\"A\"],  # Annual frequency\n    indicator=[\"WPI\"],  # Wholesale Price Index\n    type_of_transformation=[\"IX\"],\n    start_year=2000,\n    end_year=2015\n)\n\nThis example creates two objects we’ll use in the following sections:\n\nparams: A dictionary of parameters and their valid codes\ndf: The retrieved data frame containing our requested data"
  },
  {
    "objectID": "docs/datasets.html#making-a-request",
    "href": "docs/datasets.html#making-a-request",
    "title": "Requesting Datasets",
    "section": "",
    "text": "To retrieve data from an IMF database, you’ll need the database ID and any relevant filter parameters. Here’s a basic example using the Producer Price Index (PPI) database from 2000 to 2015:\n\nimport imfp\n\n# Get parameters and their valid codes\nparams = imfp.imf_parameters(\"PPI\")\n\n# Fetch annual coal price index data\ndf = imfp.imf_dataset(\n    database_id=\"PPI\",\n    frequency=[\"A\"],  # Annual frequency\n    indicator=[\"WPI\"],  # Wholesale Price Index\n    type_of_transformation=[\"IX\"],\n    start_year=2000,\n    end_year=2015\n)\n\nThis example creates two objects we’ll use in the following sections:\n\nparams: A dictionary of parameters and their valid codes\ndf: The retrieved data frame containing our requested data"
  },
  {
    "objectID": "docs/datasets.html#decoding-returned-data",
    "href": "docs/datasets.html#decoding-returned-data",
    "title": "Requesting Datasets",
    "section": "Decoding Returned Data",
    "text": "Decoding Returned Data\nWhen you retrieve data using imf_dataset, the returned data frame contains columns that correspond to the parameters you specified in your request. However, these columns use input codes (short identifiers) rather than human-readable descriptions. To make your data more interpretable, you can replace these codes with their corresponding text descriptions using the parameter information from imf_parameters, so that codes like “A” (Annual) or “W00” (World) become self-explanatory labels.\nFor example, suppose we want to decode the frequency, country, and type_of_transformation (unit) columns in our dataset. We’ll merge the parameter descriptions into our data frame:\n\n# Decode frequency codes (e.g., \"A\" → \"Annual\")\ndecoded_df = df.merge(\n    # Select code-description pairs\n    params['frequency'][['input_code', 'description']],\n    # Match codes in the data frame\n    left_on='frequency',\n    # ...to codes in the parameter data\n    right_on='input_code',\n    # Keep all data rows\n    how='left'\n).drop(columns=['frequency', 'input_code']\n).rename(columns={\"description\": \"frequency\"})\n\n# Decode geographic area codes (e.g., \"W00\" → \"World\")\ndecoded_df = decoded_df.merge(\n    params['country'][['input_code', 'description']],\n    left_on='country',\n    right_on='input_code',\n    how='left'\n).drop(columns=['country', 'input_code']\n).rename(columns={\"description\":\"country\"})\n\n# Decode unit codes (e.g., \"IX\" → \"Index\")\ndecoded_df = decoded_df.merge(\n    params['type_of_transformation'][['input_code', 'description']],\n    left_on='type_of_transformation',\n    right_on='input_code',\n    how='left'\n).drop(columns=['type_of_transformation', 'input_code']\n).rename(columns={\"description\":\"type_of_transformation\"})\n\ndecoded_df.head()\n\n\n\n\n\n\n\n\nindicator\ntime_period\nobs_value\nfrequency\ncountry\ntype_of_transformation\n\n\n\n\n0\nWPI\n2008\n80.099924\nAnnual\nAngola\nIndex\n\n\n1\nWPI\n2009\n88.676029\nAnnual\nAngola\nIndex\n\n\n2\nWPI\n2010\n100.000000\nAnnual\nAngola\nIndex\n\n\n3\nWPI\n2011\n111.889914\nAnnual\nAngola\nIndex\n\n\n4\nWPI\n2012\n117.840781\nAnnual\nAngola\nIndex\n\n\n\n\n\n\n\nAfter decoding, the data frame is much more human-interpretable. This transformation makes the data more accessible for analysis and presentation, while maintaining all the original information."
  },
  {
    "objectID": "docs/databases.html",
    "href": "docs/databases.html",
    "title": "Working with Databases",
    "section": "",
    "text": "The IMF serves many different databases through its API, and the API needs to know which of these many databases you’re requesting data from. Before you can fetch any data, you’ll need to:\n\nGet a list of available databases\nFind the database ID for the data you want\n\nThen you can use that database ID to fetch the data."
  },
  {
    "objectID": "docs/databases.html#understanding-imf-databases",
    "href": "docs/databases.html#understanding-imf-databases",
    "title": "Working with Databases",
    "section": "",
    "text": "The IMF serves many different databases through its API, and the API needs to know which of these many databases you’re requesting data from. Before you can fetch any data, you’ll need to:\n\nGet a list of available databases\nFind the database ID for the data you want\n\nThen you can use that database ID to fetch the data."
  },
  {
    "objectID": "docs/databases.html#fetching-the-database-list",
    "href": "docs/databases.html#fetching-the-database-list",
    "title": "Working with Databases",
    "section": "Fetching the Database List",
    "text": "Fetching the Database List\n\nFetching an Index of Databases with the imf_databases Function\nTo obtain the list of available databases and their corresponding IDs, use imf_databases:\n\nimport imfp\n\n#Fetch the list of databases available through the IMF API\ndatabases = imfp.imf_databases()\ndatabases.head()\n\n\n\n\n\n\n\n\ndatabase_id\ndescription\n\n\n\n\n0\nGDD\nGlobal Debt Database (GDD)\n\n\n1\nITG_WCA\nInternational Trade in Goods, World and Countr...\n\n\n2\nSRD\nStructural Reform Database (SRD)\n\n\n3\nPPI\nProducer Price Index (PPI)\n\n\n4\nEER\nEffective Exchange Rate (EER)\n\n\n\n\n\n\n\nThis function returns the IMF’s listing of 71 databases available through the API."
  },
  {
    "objectID": "docs/databases.html#exploring-the-database-list",
    "href": "docs/databases.html#exploring-the-database-list",
    "title": "Working with Databases",
    "section": "Exploring the Database List",
    "text": "Exploring the Database List\nTo view and explore the database list, it’s possible to explore subsets of the data frame by row number with databases.loc:\n\n# View a subset consisting of rows 5 through 9\ndatabases.loc[5:9]\n\n\n\n\n\n\n\n\ndatabase_id\ndescription\n\n\n\n\n5\nPSBS\nPublic Sector Balance Sheet (PSBS)\n\n\n6\nMFS_OFC\nMonetary and Financial Statistics (MFS), Other...\n\n\n7\nEQ\nExport Quality (EQ)\n\n\n8\nISORA_2016_DATA_PUB\nISORA 2016 Data\n\n\n9\nFA\nFund Accounts (FA)\n\n\n\n\n\n\n\nOr, if you already know which database you want, you can fetch the corresponding code by searching for a string match using str.contains and subsetting the data frame for matching rows. For instance, here’s how to search for commodities data:\n\ndatabases[databases['description'].str.contains(\"Commodity\")]\n\n\n\n\n\n\n\n\ndatabase_id\ndescription\n\n\n\n\n43\nPCPS\nPrimary Commodity Price System (PCPS)\n\n\n58\nCTOT\nCommodity Terms of Trade (CTOT)\n\n\n\n\n\n\n\nSee also Working with Large Data Frames for sample code showing how to view the full contents of the data frame in a browser window."
  },
  {
    "objectID": "docs/databases.html#best-practices",
    "href": "docs/databases.html#best-practices",
    "title": "Working with Databases",
    "section": "Best Practices",
    "text": "Best Practices\n\nCache the Database List: The database list rarely changes. Consider saving it locally if you’ll be making multiple queries. See Caching Strategy for sample code.\nSearch Strategically: Use specific search terms to find relevant databases. For example:\n\n“Price” for price indices\n“Trade” for trade statistics\n“Financial” for financial data\n\nUse a Browser Viewer: See Working with Large Data Frames for sample code showing how to view the full contents of the data frame in a browser window.\nNote Database IDs: Once you find a database you’ll use frequently, note its database ID for future reference."
  },
  {
    "objectID": "docs/databases.html#next-steps",
    "href": "docs/databases.html#next-steps",
    "title": "Working with Databases",
    "section": "Next Steps",
    "text": "Next Steps\nOnce you’ve identified the database you want to use, you’ll need to:\n\nGet the list of parameters for that database (see Parameters)\nUse those parameters to fetch your data (see Datasets)"
  },
  {
    "objectID": "docs/demo.html",
    "href": "docs/demo.html",
    "title": "Economic Growth and Gender Equality: An Analysis Using IMF Data",
    "section": "",
    "text": "This data analysis project aims to explore the relationship between economic growth and gender equality using imfp, which allows us to download data from IMF (International Monetary Fund). imfp can be integrated with other python tools to streamline the computational process. To demonstrate its functionality, the project experimented with a variety of visualization and analysis methods."
  },
  {
    "objectID": "docs/demo.html#executive-summary",
    "href": "docs/demo.html#executive-summary",
    "title": "Economic Growth and Gender Equality: An Analysis Using IMF Data",
    "section": "Executive Summary",
    "text": "Executive Summary\nIn this project, we explored the following:\n\nData Fetching\n\n\nMake API call to fetch 4 datasets: GII (Gender Inequality Index), Nominal GDP, GDP Deflator Index, Population series\n\n\nFeature Engineering\n\n\nCleaning: Convert GDP Deflator Index to a yearly basis and variables to numeric\nDependent Variable: Percent Change of Gender Inequality Index\nIndependent Variable: Percent Change of Real GDP per Capita\nTransform variables to display magnitude of change\nMerge the datasets\n\n\nData Visualization\n\n\nScatterplot\nTime Series Line Plots\nBarplot\nBoxplot\nHeatmap\n\n\nStatistical Analysis\n\n\nDescriptive Statistics\nRegression Analysis\nTime Series Analysis"
  },
  {
    "objectID": "docs/demo.html#utility-functions",
    "href": "docs/demo.html#utility-functions",
    "title": "Economic Growth and Gender Equality: An Analysis Using IMF Data",
    "section": "Utility Functions",
    "text": "Utility Functions\nThe integration of other Python tools not only streamlined our computational processes but also ensured consistency across the project.\nA custom module is written to simplify the process of making API calls and fetching information with imfp library. load_or_fetch_databases, load_or_fetch_parameters load_or_fetch_dataset load and retreive database, parameters, and dataset from a local or remote source. view_dataframe_in_browser displays dataframe in a web browser.\n\nimport os\nimport pickle\nfrom tempfile import NamedTemporaryFile\nimport pandas as pd\nimport imfp\nimport webbrowser\n\n\n# Function to display a DataFrame in a web browser\ndef view_dataframe_in_browser(df):\n    html = df.to_html()\n    with NamedTemporaryFile(delete=False, mode=\"w\", suffix=\".html\") as f:\n        url = \"file://\" + f.name\n        f.write(html)\n    webbrowser.open(url)\n\n\n# Function to load databases from CSV or fetch from API\ndef load_or_fetch_databases():\n    csv_path = os.path.join(\"data\", \"databases.csv\")\n\n    # Try to load from CSV\n    if os.path.exists(csv_path):\n        try:\n            return pd.read_csv(csv_path)\n        except Exception as e:\n            print(f\"Error loading CSV: {e}\")\n\n    # If CSV doesn't exist or couldn't be loaded, fetch from API\n    print(\"Fetching databases from IMF API...\")\n    databases = imfp.imf_databases()\n\n    # Save to CSV for future use\n    databases.to_csv(csv_path, index=False)\n    print(f\"Databases saved to {csv_path}\")\n\n    return databases\n\n\ndef load_or_fetch_parameters(database_name):\n    pickle_path = os.path.join(\"data\", f\"{database_name}.pickle\")\n\n    # Try to load from pickle file\n    if os.path.exists(pickle_path):\n        try:\n            with open(pickle_path, \"rb\") as f:\n                return pickle.load(f)\n        except Exception as e:\n            print(f\"Error loading pickle file: {e}\")\n\n    # If pickle doesn't exist or couldn't be loaded, fetch from API\n    print(f\"Fetching parameters for {database_name} from IMF API...\")\n    parameters = imfp.imf_parameters(database_name)\n\n    # Save to pickle file for future use\n    os.makedirs(\"data\", exist_ok=True)  # Ensure the data directory exists\n    with open(pickle_path, \"wb\") as f:\n        pickle.dump(parameters, f)\n    print(f\"Parameters saved to {pickle_path}\")\n\n    return parameters\n\n\ndef load_or_fetch_dataset(database_id, indicator):\n    file_name = f\"{database_id}.{indicator}.csv\"\n    csv_path = os.path.join(\"data\", file_name)\n\n    # Try to load from CSV file\n    if os.path.exists(csv_path):\n        try:\n            return pd.read_csv(csv_path)\n        except Exception as e:\n            print(f\"Error loading CSV file: {e}\")\n\n    # If CSV doesn't exist or couldn't be loaded, fetch from API\n    print(f\"Fetching dataset for {database_id}.{indicator} from IMF API...\")\n    dataset = imfp.imf_dataset(database_id=database_id, indicator=[indicator])\n\n    # Save to CSV file for future use\n    os.makedirs(\"data\", exist_ok=True)  # Ensure the data directory exists\n    dataset.to_csv(csv_path, index=False)\n    print(f\"Dataset saved to {csv_path}\")\n\n    return dataset"
  },
  {
    "objectID": "docs/demo.html#dependencies",
    "href": "docs/demo.html#dependencies",
    "title": "Economic Growth and Gender Equality: An Analysis Using IMF Data",
    "section": "Dependencies",
    "text": "Dependencies\nHere is a brief introduction about the packages used:\npandas: view and manipulate data frame\nmatplotlib.pyplot: make plots\nseaborn: make plots\nnumpy: computation\nLinearRegression: implement linear regression\ntabulate: format data into tables\nstatsmodels.api, adfuller, ARIMA,VAR,plot_acf,plot_pacf,mean_absolute_error,mean_squared_error, andgrangercausalitytests are specifically used for time series analysis.\npycountry: convert between ISO2 and ISO3 country codes (install with pip install pycountry)\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\nfrom tabulate import tabulate\nimport statsmodels.api as sm\nfrom statsmodels.tsa.stattools import adfuller\nfrom statsmodels.tsa.arima.model import ARIMA\nfrom statsmodels.tsa.vector_ar.var_model import VAR\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import mean_squared_error\nfrom statsmodels.tsa.stattools import grangercausalitytests\nimport pycountry"
  },
  {
    "objectID": "docs/demo.html#data-fetching",
    "href": "docs/demo.html#data-fetching",
    "title": "Economic Growth and Gender Equality: An Analysis Using IMF Data",
    "section": "Data Fetching",
    "text": "Data Fetching\nIn this section, we load four datasets: Gender Inequality Index (GII) from a CSV file, and fetch GDP Deflator, Nominal GDP, and Population through API calls to the IMF.\n\nfrom pathlib import Path\nPath(\"data\").mkdir(exist_ok=True)\n\n\n# Load or fetch databases\ndatabases = load_or_fetch_databases()\n\n# Filter out databases that contain a year in the description\ndatabases[\n  ~databases['description'].str.contains(r\"[\\d]{4}\", regex=True)\n]\n\n# view_dataframe_in_browser(databases)\n\nFetching databases from IMF API...\nDatabases saved to data/databases.csv\n\n\n\n\n\n\n\n\n\ndatabase_id\ndescription\n\n\n\n\n0\nQGDP_WCA\nQuarterly Gross Domestic Product (GDP), World ...\n\n\n1\nSPE\nSpecial Purpose Entities (SPEs)\n\n\n2\nWPCPER\nCrypto-based Parallel Exchange Rates (Working ...\n\n\n3\nMFS_IR\nMonetary and Financial Statistics (MFS), Inter...\n\n\n4\nGFS_SSUC\nGFS Statement of Sources and Uses of Cash\n\n\n...\n...\n...\n\n\n66\nFM\nFiscal Monitor (FM)\n\n\n67\nHPD\nHistorical Public Debt (HPD)\n\n\n68\nFSICDM\nFinancial Soundness Indicators (FSI), Concentr...\n\n\n69\nQGFS\nQuarterly Government Finance Statistics (QGFS)\n\n\n70\nIRFCL\nInternational Reserves and Foreign Currency Li...\n\n\n\n\n69 rows × 2 columns\n\n\n\nThree IMF databases were used: World Economic Outlook (WEO), and National Economic Accounts (QNEA, ANEA).\n\n\n\n\n\n\nNoteDatabase Changes\n\n\n\nThe IMF has updated their API structure. The former IFS (International Financial Statistics) database has been discontinued and replaced with more specialized databases. This demo now uses:\n\nQNEA: Quarterly National Economic Accounts for GDP deflator\nANEA: Annual National Economic Accounts for nominal GDP\nWEO: World Economic Outlook for population data\n\nNote: The Gender Inequality Index (GII) is a UN dataset that is no longer available through the IMF API. We’ll load it from a pre-downloaded CSV file instead.\n\n\n\ndatabases[databases['database_id'].isin(['QNEA','ANEA','WEO'])]\n\n\n\n\n\n\n\n\ndatabase_id\ndescription\n\n\n\n\n13\nQNEA\nNational Economic Accounts (NEA), Quarterly Data\n\n\n57\nWEO\nWorld Economic Outlook (WEO)\n\n\n63\nANEA\nNational Economic Accounts (NEA), Annual Data\n\n\n\n\n\n\n\nParameters are dictionary key names to make requests from the databases. “country” is the ISO-3 code of the country. “indicator” refers to the code representing a specific dataset in the database.\n\ndatasets_list = [\"QNEA\", \"ANEA\", \"WEO\"]\nparams = {}\n\n# Fetch valid parameters for datasets\nfor dataset in datasets_list:\n    params[dataset] = load_or_fetch_parameters(dataset)\n\n    valid_keys = list(params[dataset].keys())\n    print(f\"Parameters for {dataset}: \", valid_keys)\n\nFetching parameters for QNEA from IMF API...\nParameters saved to data/QNEA.pickle\nParameters for QNEA:  ['country', 'indicator', 'price_type', 's_adjustment', 'type_of_transformation', 'frequency']\nFetching parameters for ANEA from IMF API...\nParameters saved to data/ANEA.pickle\nParameters for ANEA:  ['country', 'indicator', 'price_type', 'type_of_transformation', 'frequency']\nFetching parameters for WEO from IMF API...\nParameters saved to data/WEO.pickle\nParameters for WEO:  ['country', 'indicator', 'frequency']\n\n\nWe’ll need to update the load_or_fetch_dataset function to handle the new database parameters:\n\ndef load_or_fetch_dataset_with_params(database_id, filename_suffix, **kwargs):\n    \"\"\"Fetch dataset with flexible parameters for new IMF databases\"\"\"\n    file_name = f\"{database_id}.{filename_suffix}.csv\"\n    csv_path = os.path.join(\"data\", file_name)\n\n    # Try to load from CSV file\n    if os.path.exists(csv_path):\n        try:\n            return pd.read_csv(csv_path)\n        except Exception as e:\n            print(f\"Error loading CSV file: {e}\")\n\n    # If CSV doesn't exist or couldn't be loaded, fetch from API\n    print(f\"Fetching dataset for {database_id}.{filename_suffix} from IMF API...\")\n    dataset = imfp.imf_dataset(database_id=database_id, **kwargs)\n\n    # Save to CSV file for future use\n    os.makedirs(\"data\", exist_ok=True)\n    dataset.to_csv(csv_path, index=False)\n    print(f\"Dataset saved to {csv_path}\")\n\n    return dataset\n\ndef convert_iso2_to_iso3(iso2_code):\n    \"\"\"Convert ISO2 country code to ISO3\"\"\"\n    try:\n        # Handle NaN or non-string values\n        if pd.isna(iso2_code) or not isinstance(iso2_code, str):\n            return None\n        country = pycountry.countries.get(alpha_2=iso2_code.upper())\n        return country.alpha_3 if country else None\n    except (KeyError, AttributeError, LookupError):\n        return None\n\nNow fetch the datasets using the new databases:\n\n# Gender Inequality Index - Load from CSV (UN dataset, not available via IMF API)\nGII_data = pd.read_csv(\"data/GENDER_EQUALITY.GE_GII.csv\")\n\n# Convert ISO2 country codes (ref_area) to ISO3 (country) to match IMF datasets\nGII_data['country'] = GII_data['ref_area'].apply(convert_iso2_to_iso3)\n\n# Drop rows where country code conversion failed\nGII_data = GII_data.dropna(subset=['country'])\n\nprint(f\"Loaded {len(GII_data)} GII observations\")\nprint(f\"Sample country codes: {GII_data['country'].unique()[:5]}\")\n\n# GDP Deflator (Quarterly, Index)\nGDP_deflator_data = load_or_fetch_dataset_with_params(\n    \"QNEA\",\n    \"B1GQ_PD_IX\",\n    indicator=\"B1GQ\",\n    price_type=\"PD\",  # Price deflator\n    type_of_transformation=\"IX\",  # Index\n    frequency=\"Q\"\n)\n\n# Nominal GDP (Annual, Domestic Currency)\nGDP_nominal_data = load_or_fetch_dataset_with_params(\n    \"ANEA\",\n    \"B1GQ_V_XDC\",\n    indicator=\"B1GQ\",\n    price_type=\"V\",  # Current prices\n    type_of_transformation=\"XDC\",  # Domestic currency\n    frequency=\"A\"\n)\n\n# Population (Annual)\nGDP_population_data = load_or_fetch_dataset_with_params(\n    \"WEO\",\n    \"LP\",\n    indicator=[\"LP\"],\n    frequency=\"A\"\n)\n\nLoaded 3065 GII observations\nSample country codes: ['AFG' 'ALB' 'DZA' 'ARG' 'ARM']\nFetching dataset for QNEA.B1GQ_PD_IX from IMF API...\n\n\n/tmp/ipykernel_7431/2514471997.py:5: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\nYou are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\nA typical example is when you are setting values in a column of a DataFrame, like:\n\ndf[\"col\"][row_indexer] = value\n\nUse `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n  GII_data['country'] = GII_data['ref_area'].apply(convert_iso2_to_iso3)\n\n\nDataset saved to data/QNEA.B1GQ_PD_IX.csv\nFetching dataset for ANEA.B1GQ_V_XDC from IMF API...\nDataset saved to data/ANEA.B1GQ_V_XDC.csv\nFetching dataset for WEO.LP from IMF API...\nDataset saved to data/WEO.LP.csv"
  },
  {
    "objectID": "docs/demo.html#feature-engineering",
    "href": "docs/demo.html#feature-engineering",
    "title": "Economic Growth and Gender Equality: An Analysis Using IMF Data",
    "section": "Feature Engineering",
    "text": "Feature Engineering\n\nData Cleaning\nSince the GDP deflator was reported on a quarterly basis, we converted it to a yearly basis.\n\n# Keep only rows with a partial string match for \"Q4\" in the time_period column\nGDP_deflator_data = GDP_deflator_data[GDP_deflator_data\n['time_period'].str.contains(\"Q4\")]\n\n\n# Split the time_period into year and quarter and keep the year only\nGDP_deflator_data.loc[:, 'time_period'] = GDP_deflator_data['time_period'].str[0:4]\n\nWe make all the variables numeric.\n\n\n\n\n\n\nNoteUnit Multiplier Changes\n\n\n\nThe new IMF API returns values in the correct units, so we no longer need to apply unit multipliers. The unit_mult column has been removed from most datasets.\n\n\n\ndatasets = [GII_data, GDP_deflator_data, GDP_nominal_data, GDP_population_data]\n\nfor i, dataset in enumerate(datasets):    \n    # Use .loc to modify the columns\n    datasets[i].loc[:, 'obs_value'] = pd.to_numeric(datasets[i]['obs_value'], \n    errors='coerce')\n    datasets[i].loc[:, 'time_period'] = pd.to_numeric(datasets[i]['time_period'], \n    errors='coerce')\n\n\n\nGII Percent Change: Dependent Variable\nWe kept percents as decimals to make them easy to work with for calculation. Different countries have different baseline level of economic growth and gender equality. We calculated the percent change to make them comparable.\nGender Inequality Index (GII) is a composite measure of gender inequality using three dimensions: reproductive health, empowerment, and labor market. GII ranges from 0 to 1. While 0 indicates gender equality, 1 indicates gender inequality, possibly the worst outcome for one gender in all three dimensions.\n\n# Calculate percent change for each country\n# First, create a copy and reset the index to avoid duplicate index issues\nGII_data_sorted = GII_data.sort_values(\n    ['country', 'time_period']).reset_index(drop=True)\nGII_data['pct_change'] = GII_data_sorted.groupby('country')['obs_value'].pct_change()\n\n# Display the first few rows of the updated dataset\nGII_data.head()\n\n\n\n\n\n\n\n\nfreq\nref_area\nindicator\nunit_mult\ntime_format\ntime_period\nobs_value\ncountry\npct_change\n\n\n\n\n0\nA\nAF\nGE_GII\n0\nP1Y\n1990\n0.828244\nAFG\nNaN\n\n\n1\nA\nAF\nGE_GII\n0\nP1Y\n1991\n0.817706\nAFG\n-0.012723\n\n\n2\nA\nAF\nGE_GII\n0\nP1Y\n1992\n0.809806\nAFG\n-0.009662\n\n\n3\nA\nAF\nGE_GII\n0\nP1Y\n1993\n0.803078\nAFG\n-0.008308\n\n\n4\nA\nAF\nGE_GII\n0\nP1Y\n1994\n0.797028\nAFG\n-0.007533\n\n\n\n\n\n\n\nWe subset the data frame to keep only the columns we want:\n\n# Create a new dataframe with only the required columns\nGII_data = GII_data[['country', 'time_period', 'obs_value', 'pct_change']].copy()\n\nGII_data = GII_data.rename(columns = {\n    'country': 'Country',\n    'time_period': 'Time',\n    'obs_value': 'GII',\n    'pct_change': 'GII_change'\n})\n\n# Display the first few rows of the new dataset\nGII_data.head()\n\n\n\n\n\n\n\n\nCountry\nTime\nGII\nGII_change\n\n\n\n\n0\nAFG\n1990\n0.828244\nNaN\n\n\n1\nAFG\n1991\n0.817706\n-0.012723\n\n\n2\nAFG\n1992\n0.809806\n-0.009662\n\n\n3\nAFG\n1993\n0.803078\n-0.008308\n\n\n4\nAFG\n1994\n0.797028\n-0.007533\n\n\n\n\n\n\n\n\n\nGDP Percent Change: Independent Variable\nReal GDP per capita is a measure of a country’s economic welfare or standard of living. It is a great tool comparing a country’s economic development compared to other economies. Due to dataset access issue, we calculated Real GDP per capita by the following formula using GDP Deflator, Nominal GDP, and Population data:\n\\(\\text{Real GDP} = \\frac{\\text{Nominal GDP}}{\\text{GDP Deflator Index}}\\times 100\\)\n\\(\\text{Real GDP per capita} = \\frac{\\text{Real GDP}}{\\text{Population}}\\)\nGDP Deflator is a measure of price inflation and deflation with respect to a specific base year. The GDP deflator of a base year is equal to 100. A number of 200 indicates price inflation: the current year price of the good is twice its base year price. A number of 50 indicates price deflation: the current year price of the good is half its base year price. We kept the columns we want only for GDP-related datasets for easier table merging.\n\n# GDP Deflator Dataset\n# Create a new dataframe with only the required columns\nGDP_deflator_data = GDP_deflator_data[\n    ['country', 'time_period', 'obs_value']].copy()\n\n# Display the first few rows of the new dataset\nGDP_deflator_data.head()\n\n\n\n\n\n\n\n\ncountry\ntime_period\nobs_value\n\n\n\n\n3\nALB\n1996\n55.229150\n\n\n7\nALB\n1997\n60.439943\n\n\n11\nALB\n1998\n63.745473\n\n\n15\nALB\n1999\n67.101488\n\n\n19\nALB\n2000\n70.301830\n\n\n\n\n\n\n\nNominal GDP is the total value of all goods and services produced in a given time period. It is usually higher than Real GDP and does not take into account cost of living in different countries or price change due to inflation/deflation.\n\n# GDP Nominal Data\n# Create a new dataframe with only the required columns\nGDP_nominal_data = GDP_nominal_data[\n    ['country', 'time_period', 'obs_value']].copy()\n\n# Display the first few rows of the new dataset\nGDP_nominal_data.head()\n\n\n\n\n\n\n\n\ncountry\ntime_period\nobs_value\n\n\n\n\n0\nAFG\n2010\n7.035025e+11\n\n\n1\nAFG\n2011\n8.378513e+11\n\n\n2\nAFG\n2012\n1.007959e+12\n\n\n3\nAFG\n2013\n1.102256e+12\n\n\n4\nAFG\n2014\n1.116353e+12\n\n\n\n\n\n\n\nPopulation is the total number of people living in a country at a given time. This is where the “per capita” comes from. Real GDP is the total value of all goods and services produced in a country adjusted for inflation. Real GDP per capita is the total economic output per person in a country.\n\n# GDP Population Data \n# Create a new dataframe with only the required columns\nGDP_population_data = GDP_population_data[\n    ['country', 'time_period', 'obs_value']].copy()\n\n# Display the first few rows of the new dataset\nGDP_population_data.head()\n\n\n\n\n\n\n\n\ncountry\ntime_period\nobs_value\n\n\n\n\n0\nABW\n1986\n81000.0\n\n\n1\nABW\n1987\n85000.0\n\n\n2\nABW\n1988\n88000.0\n\n\n3\nABW\n1989\n89000.0\n\n\n4\nABW\n1990\n90000.0\n\n\n\n\n\n\n\n\n# Combine all the datasets above for further calculation\nmerged_df = pd.merge(pd.merge(GDP_deflator_data, GDP_nominal_data, \non=['time_period', 'country'], \nsuffixes=('_index', '_nominal'), \nhow='inner'), \nGDP_population_data, \non=['time_period', 'country'], \nhow='inner')\n\n# Rename columns for clarity\nmerged_df = merged_df.rename(columns={\n    'obs_value_index': 'deflator',\n    'obs_value_nominal': 'nominal',\n    'obs_value': 'population'\n})\n\n# Display the first few rows of the dataset\nmerged_df.head()\n\n\n\n\n\n\n\n\ncountry\ntime_period\ndeflator\nnominal\npopulation\n\n\n\n\n0\nALB\n1996\n55.229150\n3.380003e+11\n3168000.0\n\n\n1\nALB\n1997\n60.439943\n3.364808e+11\n3148000.0\n\n\n2\nALB\n1998\n63.745473\n3.930700e+11\n3129000.0\n\n\n3\nALB\n1999\n67.101488\n4.535123e+11\n3109000.0\n\n\n4\nALB\n2000\n70.301830\n5.162068e+11\n3089000.0\n\n\n\n\n\n\n\nWe wanted to compute the Real GDP per capita.\n\n# Step 1: Real GDP = (Nominal GDP / GDP Deflator Index)*100\nmerged_df['Real_GDP_domestic'] = (merged_df['nominal'] / merged_df[\n    'deflator'])*100\n\n# Step 2: Real GDP per Capita = Real GDP / Population\nmerged_df['Real_GDP_per_capita'] = merged_df['Real_GDP_domestic'] / merged_df[\n    'population']\n\n# Rename columns\nmerged_df = merged_df.rename(columns= {\n    \"country\": \"Country\",\n    \"time_period\": \"Time\",\n    \"nominal\": \"Nominal\",\n    \"deflator\": \"Deflator\",\n    \"population\": \"Population\",\n    \"Real_GDP_domestic\": \"Real GDP\",\n    \"Real_GDP_per_capita\": \"Real GDP per Capita\"\n}\n)\n# Check the results\nmerged_df.head()\n\n/tmp/ipykernel_7431/1437906122.py:2: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\nYou are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\nA typical example is when you are setting values in a column of a DataFrame, like:\n\ndf[\"col\"][row_indexer] = value\n\nUse `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n  merged_df['Real_GDP_domestic'] = (merged_df['nominal'] / merged_df[\n/tmp/ipykernel_7431/1437906122.py:6: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\nYou are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\nA typical example is when you are setting values in a column of a DataFrame, like:\n\ndf[\"col\"][row_indexer] = value\n\nUse `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n  merged_df['Real_GDP_per_capita'] = merged_df['Real_GDP_domestic'] / merged_df[\n\n\n\n\n\n\n\n\n\nCountry\nTime\nDeflator\nNominal\nPopulation\nReal GDP\nReal GDP per Capita\n\n\n\n\n0\nALB\n1996\n55.229150\n3.380003e+11\n3168000.0\n6.119962e+11\n193180.623583\n\n\n1\nALB\n1997\n60.439943\n3.364808e+11\n3148000.0\n5.567193e+11\n176848.560331\n\n\n2\nALB\n1998\n63.745473\n3.930700e+11\n3129000.0\n6.166241e+11\n197067.474349\n\n\n3\nALB\n1999\n67.101488\n4.535123e+11\n3109000.0\n6.758603e+11\n217388.325042\n\n\n4\nALB\n2000\n70.301830\n5.162068e+11\n3089000.0\n7.342722e+11\n237705.465473\n\n\n\n\n\n\n\nWe calculated the percentage change in Real GDP per capita and put it in a new column.\n\n# Calculate percent change for each country\nmerged_df[f'GDP_change'] = merged_df.sort_values(['Country', 'Time']).groupby(\n    'Country')['Real GDP per Capita'].pct_change()\n\n# Rename dataset\nGDP_data = merged_df\n\n# Display the first few rows of the dataset\nGDP_data.head()\n\n/tmp/ipykernel_7431/776684234.py:2: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\nYou are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\nA typical example is when you are setting values in a column of a DataFrame, like:\n\ndf[\"col\"][row_indexer] = value\n\nUse `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n  merged_df[f'GDP_change'] = merged_df.sort_values(['Country', 'Time']).groupby(\n\n\n\n\n\n\n\n\n\nCountry\nTime\nDeflator\nNominal\nPopulation\nReal GDP\nReal GDP per Capita\nGDP_change\n\n\n\n\n0\nALB\n1996\n55.229150\n3.380003e+11\n3168000.0\n6.119962e+11\n193180.623583\nNaN\n\n\n1\nALB\n1997\n60.439943\n3.364808e+11\n3148000.0\n5.567193e+11\n176848.560331\n-0.084543\n\n\n2\nALB\n1998\n63.745473\n3.930700e+11\n3129000.0\n6.166241e+11\n197067.474349\n0.114329\n\n\n3\nALB\n1999\n67.101488\n4.535123e+11\n3109000.0\n6.758603e+11\n217388.325042\n0.103116\n\n\n4\nALB\n2000\n70.301830\n5.162068e+11\n3089000.0\n7.342722e+11\n237705.465473\n0.093460\n\n\n\n\n\n\n\n\n# GII and GDP\n# Merge the datasets\ncombined_data = pd.merge(GII_data, GDP_data, \non=[\"Country\", \"Time\"], \nhow = \"inner\")\n\n# Check the combined dataset\ncombined_data.head()\n\n/home/runner/work/imfp/imfp/.venv/lib/python3.14/site-packages/pandas/core/reshape/merge.py:1548: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\nYou are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\nA typical example is when you are setting values in a column of a DataFrame, like:\n\ndf[\"col\"][row_indexer] = value\n\nUse `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n  self.left[name] = self.left[name].astype(typ)\n\n\n\n\n\n\n\n\n\nCountry\nTime\nGII\nGII_change\nDeflator\nNominal\nPopulation\nReal GDP\nReal GDP per Capita\nGDP_change\n\n\n\n\n0\nALB\n1996\n0.340120\n0.031715\n55.229150\n3.380003e+11\n3168000.0\n6.119962e+11\n193180.623583\nNaN\n\n\n1\nALB\n1997\n0.352818\n0.037334\n60.439943\n3.364808e+11\n3148000.0\n5.567193e+11\n176848.560331\n-0.084543\n\n\n2\nALB\n1998\n0.368950\n0.045723\n63.745473\n3.930700e+11\n3129000.0\n6.166241e+11\n197067.474349\n0.114329\n\n\n3\nALB\n1999\n0.393371\n0.066190\n67.101488\n4.535123e+11\n3109000.0\n6.758603e+11\n217388.325042\n0.103116\n\n\n4\nALB\n2000\n0.390317\n-0.007762\n70.301830\n5.162068e+11\n3089000.0\n7.342722e+11\n237705.465473\n0.093460"
  },
  {
    "objectID": "docs/demo.html#data-visualization",
    "href": "docs/demo.html#data-visualization",
    "title": "Economic Growth and Gender Equality: An Analysis Using IMF Data",
    "section": "Data Visualization",
    "text": "Data Visualization\n\nScatterplot\nScatterplot use dots to represent values of two numeric variables. The horizontal axis was the percent change in Real GDP per capita. The vertical axis was the percent change in Gender Inequality Index(GII). Different colors represented different countries. We used a linear regression line to display the overall pattern.\nBased on the scatterplot, it seemed like there was a slight positive relationship between GDP change and GII change as shown by the flat regression line. Gender inequality was decreasing (gender equality was improving) a little faster in country-years with low GDP growth and a little slower in country-years with high GDP growth.\n\n# Convert numeric columns to float\nnumeric_columns = [\n    'GII', 'GII_change', 'Nominal', 'Deflator', 'Population', \n    'Real GDP', 'Real GDP per Capita', 'GDP_change'\n]\nfor col in numeric_columns:\n    combined_data[col] = pd.to_numeric(combined_data[col], errors='coerce')\n\n# Count NAs\nprint(f\"Dropping {combined_data[numeric_columns].isna().sum()} rows with NAs\")\n\n# Drop NAs\ncombined_data = combined_data.dropna(subset=numeric_columns)\n\n# Plot the data points\nplt.figure(figsize=(8, 6))\nfor country in combined_data['Country'].unique():\n    country_data = combined_data[combined_data['Country'] == country]\n    plt.scatter(country_data['GDP_change'], country_data['GII_change'],\n             marker='o',linestyle='-', label=country)\nplt.title('Country-Year Analysis of GDP Change vs. GII Change')\nplt.xlabel('Percent Change in Real GDP per Capita (Country-Year)')\nplt.ylabel('Percent Change in GII (Country-Year)')\nplt.grid(True)\n\n# Prepare data for linear regression\nX = combined_data['GDP_change'].values.reshape(-1, 1)\ny = combined_data['GII_change'].values\n\n# Perform linear regression\nreg = LinearRegression().fit(X, y)\ny_pred = reg.predict(X)\n\n# Plot the regression line\nplt.plot(combined_data['GDP_change'], y_pred, color='red', linewidth=2)\n\nplt.show()\n\nDropping GII                     0\nGII_change             42\nNominal                 0\nDeflator                0\nPopulation              0\nReal GDP                0\nReal GDP per Capita     0\nGDP_change             39\ndtype: int64 rows with NAs\n\n\n/tmp/ipykernel_7431/207802317.py:7: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\nYou are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\nA typical example is when you are setting values in a column of a DataFrame, like:\n\ndf[\"col\"][row_indexer] = value\n\nUse `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n  combined_data[col] = pd.to_numeric(combined_data[col], errors='coerce')\n\n\n\n\n\n\n\n\n\n\n\nTime Series Line Plot\nWe created separate line plots for GDP change and GII change over time for a few key countries might show the trends more clearly.\nUS: United States\nJP: Japan\nGB: United Kindom\nFR: France\nMX: Mexico\nBased on the line plots, we saw GDP change and GII change have different patterns. For example, in Mexico, when there was a big change in real GDP per captia in 1995, the change in GII was pretty stable.\n\n# Time Series Line plot for a few key countries\nselected_countries  = ['US', 'JP', 'GB', 'FR', 'MX']\ncombined_data_selected = combined_data[combined_data['Country'].isin(selected_countries)]\n\n# Set up the Plot Structure\nfig, ax = plt.subplots(2, 1, figsize=(8, 6), sharex=True)\n\n# Plot change in real GDP per capita over time\nsns.lineplot(data = combined_data_selected, \nx = \"Time\", \ny = \"GDP_change\", \nhue = \"Country\", \nax = ax[0])\nax[0].set_title(\"Percent Change in Real GDP per Capita Over Time\")\nax[0].set_ylabel(\"Percent Change in Real GDP per Capita\")\n\n# Plot change in GII over time\nsns.lineplot(data = combined_data_selected, \nx = \"Time\", \ny = \"GII_change\", \nhue = \"Country\", \nax = ax[1])\nax[1].set_title(\"Percent Change in GII over Time\")\nax[1].set_xlabel(\"Time\")\nax[1].set_ylabel(\"GII\")\n\nplt.tight_layout\nplt.show()\n\n\n\n\n\n\n\n\n\n\nBarplot\nWe used a barplot to show average changes in GII and GDP percent change for each country to visualize regions where inequality was improving or worsening.\nThis plot supported our previous observation how GII change seemed to be not be correlated with GDP change. We also saw that, for country SI, Solvenia, there seems to be a large improvement in gender inequality.\n\n# Barplot using average GII and GDP change\n# Calculate average change for each country\ncombined_data_avg = combined_data.groupby('Country')[\n    ['GII_change','GDP_change']].mean().reset_index()\n\n# Prepare to plot structure \nplt.figure(figsize = (18,10))\n\n# Create the barplot\ncombined_data_avg.plot(kind = 'bar', x = 'Country')\nplt.ylabel('Average Change')\nplt.xlabel('Country')\nplt.legend(['GII change', 'GDP change'])\nplt.grid(axis = 'y')\n\n# Show the plot\nplt.show()\n\n&lt;Figure size 1728x960 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n\n\nBoxplot\nWe used boxplot to visualize the distribution of GDP and GII change by country, providing information about spread, median, and potential outliers. To provide a more informative view, we sequenced countries in an ascending order by the median of percent change in GDP.\nThe boxplot displayed a slight upward trend with no obvious pattern between GDP and GII change. In coutries with higher GDP change median, they also tend to have a larger spread of the GDP change. The median of GII change remained stable regardless of the magnitude of GDP change, implying weak or no association between GDP and GII change. We observed a potential outlier for country SI, Solvenia, which may explained its large improvement in Gender inequality.\n\n# Box plot for GII and GDP change\n# Melt the dataframe to long format for combined boxplot\ncombined_data_melted = combined_data.melt(id_vars=['Country'], \nvalue_vars=['GII_change', 'GDP_change'], \nvar_name='Change_Type', \nvalue_name='Value')\n\ngdp_medians = combined_data.groupby('Country')['GDP_change'].median().sort_values()\n\ncombined_data_melted['Country'] = pd.Categorical(combined_data_melted['Country'], \ncategories=gdp_medians.index, \nordered= True)\n\n# Prepare the plot structure\nplt.figure(figsize=(8, 6))\nsns.boxplot(data = combined_data_melted, \nx = \"Country\", \ny = 'Value', \nhue = 'Change_Type')\nplt.title('Distribution of GII and GDP change by Country')\nplt.xlabel('Country')\nplt.ylabel('Change')\nplt.legend(title = 'Change Type')\n\n# Show the plot\nplt.show()\n\n/tmp/ipykernel_7431/629035303.py:10: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\nYou are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\nA typical example is when you are setting values in a column of a DataFrame, like:\n\ndf[\"col\"][row_indexer] = value\n\nUse `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n  combined_data_melted['Country'] = pd.Categorical(combined_data_melted['Country'],\n/home/runner/work/imfp/imfp/.venv/lib/python3.14/site-packages/seaborn/_base.py:1447: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\nYou are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\nA typical example is when you are setting values in a column of a DataFrame, like:\n\ndf[\"col\"][row_indexer] = value\n\nUse `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n  self.plot_data[axis] = cat_data\n/home/runner/work/imfp/imfp/.venv/lib/python3.14/site-packages/seaborn/categorical.py:403: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\nYou are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\nA typical example is when you are setting values in a column of a DataFrame, like:\n\ndf[\"col\"][row_indexer] = value\n\nUse `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n  data[\"width\"] /= n\n/home/runner/work/imfp/imfp/.venv/lib/python3.14/site-packages/seaborn/categorical.py:407: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\nYou are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\nA typical example is when you are setting values in a column of a DataFrame, like:\n\ndf[\"col\"][row_indexer] = value\n\nUse `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n  data[self.orient] += offset\n/home/runner/work/imfp/imfp/.venv/lib/python3.14/site-packages/seaborn/categorical.py:415: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\nYou are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\nA typical example is when you are setting values in a column of a DataFrame, like:\n\ndf[\"col\"][row_indexer] = value\n\nUse `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n  data[\"edge\"] = inv(data[var] - hw)\n/home/runner/work/imfp/imfp/.venv/lib/python3.14/site-packages/seaborn/categorical.py:416: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\nYou are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\nA typical example is when you are setting values in a column of a DataFrame, like:\n\ndf[\"col\"][row_indexer] = value\n\nUse `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n  data[\"width\"] = inv(data[var] + hw) - data[\"edge\"].to_numpy()\n/home/runner/work/imfp/imfp/.venv/lib/python3.14/site-packages/seaborn/categorical.py:419: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\nYou are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\nA typical example is when you are setting values in a column of a DataFrame, like:\n\ndf[\"col\"][row_indexer] = value\n\nUse `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n  data[col] = inv(data[col])\n/home/runner/work/imfp/imfp/.venv/lib/python3.14/site-packages/seaborn/categorical.py:654: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\nYou are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\nA typical example is when you are setting values in a column of a DataFrame, like:\n\ndf[\"col\"][row_indexer] = value\n\nUse `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n  stats[stat] = inv(stats[stat])\n/home/runner/work/imfp/imfp/.venv/lib/python3.14/site-packages/seaborn/categorical.py:655: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\nYou are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\nA typical example is when you are setting values in a column of a DataFrame, like:\n\ndf[\"col\"][row_indexer] = value\n\nUse `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n  stats[\"fliers\"] = stats[\"fliers\"].map(inv)\n/home/runner/work/imfp/imfp/.venv/lib/python3.14/site-packages/seaborn/categorical.py:403: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\nYou are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\nA typical example is when you are setting values in a column of a DataFrame, like:\n\ndf[\"col\"][row_indexer] = value\n\nUse `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n  data[\"width\"] /= n\n/home/runner/work/imfp/imfp/.venv/lib/python3.14/site-packages/seaborn/categorical.py:407: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\nYou are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\nA typical example is when you are setting values in a column of a DataFrame, like:\n\ndf[\"col\"][row_indexer] = value\n\nUse `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n  data[self.orient] += offset\n/home/runner/work/imfp/imfp/.venv/lib/python3.14/site-packages/seaborn/categorical.py:415: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\nYou are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\nA typical example is when you are setting values in a column of a DataFrame, like:\n\ndf[\"col\"][row_indexer] = value\n\nUse `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n  data[\"edge\"] = inv(data[var] - hw)\n/home/runner/work/imfp/imfp/.venv/lib/python3.14/site-packages/seaborn/categorical.py:416: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\nYou are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\nA typical example is when you are setting values in a column of a DataFrame, like:\n\ndf[\"col\"][row_indexer] = value\n\nUse `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n  data[\"width\"] = inv(data[var] + hw) - data[\"edge\"].to_numpy()\n/home/runner/work/imfp/imfp/.venv/lib/python3.14/site-packages/seaborn/categorical.py:419: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\nYou are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\nA typical example is when you are setting values in a column of a DataFrame, like:\n\ndf[\"col\"][row_indexer] = value\n\nUse `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n  data[col] = inv(data[col])\n/home/runner/work/imfp/imfp/.venv/lib/python3.14/site-packages/seaborn/categorical.py:654: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\nYou are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\nA typical example is when you are setting values in a column of a DataFrame, like:\n\ndf[\"col\"][row_indexer] = value\n\nUse `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n  stats[stat] = inv(stats[stat])\n/home/runner/work/imfp/imfp/.venv/lib/python3.14/site-packages/seaborn/categorical.py:655: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\nYou are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\nA typical example is when you are setting values in a column of a DataFrame, like:\n\ndf[\"col\"][row_indexer] = value\n\nUse `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n  stats[\"fliers\"] = stats[\"fliers\"].map(inv)\n\n\n\n\n\n\n\n\n\n\n\nCorrelation Matrix\nWe created a heatmap to show the relationship between GII and GDP change.\nA positive correlation coefficient indicates a positive relationship: the larger the GDP change, the larger the GII change. A negative correlation coefficient indicates a negative relationship: the larger the GDP change, the smaller the GII change. A correlation coefficient closer to 0 indicates there is weak or no relationship.\nBased on the numeric values in the plot, there was a moderately strong positive correlation between GII and GDP change for country Estonia(EE) and Ireland(IE).\n\n# Calculate the correlation\ncountry_correlation = combined_data.groupby('Country')[\n    ['GII_change', 'GDP_change']].corr().iloc[0::2, -1].reset_index(name='Correlation')\n\n# Put the correlation value in a matrix format\ncorrelation_matrix = country_correlation.pivot(index='Country', \ncolumns='level_1', \nvalues='Correlation')\n\n# Check for NaN values in the correlation matrix\n# Replace NaNs with 0 or another value as appropriate\ncorrelation_matrix.fillna(0, inplace=True)  \n\n# Set up the plot structure\n# Adjust height to give more space for y-axis labels\nplt.figure(figsize=(8, 12))  \n\n# Plot the heatmap\nsns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, \ncbar_kws={\"shrink\": .8}, \nlinewidths=.5)\n\n# Enhance axis labels and title\nplt.title('Heatmap for GII and GDP Change', fontsize=20)\nplt.xlabel('Variables', fontsize=16)\nplt.ylabel('Country', fontsize=16)\n\n# Improve readability of y-axis labels\nplt.yticks(fontsize=12)  # Adjust the font size for y-axis labels\n\n# Show the plot\nplt.show()"
  },
  {
    "objectID": "docs/demo.html#statistical-analysis",
    "href": "docs/demo.html#statistical-analysis",
    "title": "Economic Growth and Gender Equality: An Analysis Using IMF Data",
    "section": "Statistical Analysis",
    "text": "Statistical Analysis\n\nDescriptive Statistics\nThere was a total of 915 data points. The mean of the GII change in -0.0314868, which indicated the overall grand mean percent change in gender inequality index is -3.15%. The mean of the GDP change was 0.0234633, showing the overall grand mean percent change in real GDP per capita was 2.35%.\n\n# Generate summary statistics\ncombined_data.describe()\n\n\n\n\n\n\n\n\nGII\nGII_change\nDeflator\nNominal\nPopulation\nReal GDP\nReal GDP per Capita\nGDP_change\n\n\n\n\ncount\n926.000000\n926.000000\n926.000000\n9.260000e+02\n9.260000e+02\n9.260000e+02\n9.260000e+02\n926.000000\n\n\nmean\n0.238421\n-0.024551\n85.105692\n8.110427e+13\n4.417935e+07\n8.091261e+13\n1.228489e+06\n0.025226\n\n\nstd\n0.147533\n0.057831\n20.989600\n5.980526e+14\n1.244702e+08\n5.514331e+14\n4.116173e+06\n0.041567\n\n\nmin\n0.011528\n-0.755003\n3.606478\n4.005511e+09\n2.680000e+05\n5.214273e+09\n1.874604e+03\n-0.154124\n\n\n25%\n0.130789\n-0.032348\n73.539433\n1.213369e+11\n4.557500e+06\n1.634228e+11\n1.961630e+04\n0.005294\n\n\n50%\n0.187900\n-0.012143\n88.452521\n8.119985e+11\n1.027900e+07\n1.063447e+12\n3.652494e+04\n0.023782\n\n\n75%\n0.331948\n-0.003514\n100.098241\n2.591000e+12\n4.416775e+07\n2.941241e+12\n2.729200e+05\n0.044982\n\n\nmax\n0.788954\n0.597437\n207.819554\n9.546134e+15\n1.295830e+09\n7.918681e+15\n3.182519e+07\n0.218521\n\n\n\n\n\n\n\n\n\nRegression Analysis\nSimple linear regression as a foundational approach provide us with a basic understanding of the relationship between GDP change and GII change.\nBased on the summary, we concluded the following:\n\nBecasue p-value = 0.057, if we set alpha, the significance level, to be 0.05, we failed to reject the null hypothesis and conclude there was no significant relationship between percent change in real GDP per capita and gender inequality index.\nR-squared = 0.004. Only 0.4% of the variance in GII change could be explained by GDP change.\nWe were 95% confident that the interval from -0.003 to 0.169 captured the true slope of GDP change. Because 0 was included, we are uncertain about the effect of GDP change on GII chnage.\n\n\n# Get column data type summaries of combined_data\ncombined_data.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 926 entries, 1 to 1005\nData columns (total 10 columns):\n #   Column               Non-Null Count  Dtype  \n---  ------               --------------  -----  \n 0   Country              926 non-null    object \n 1   Time                 926 non-null    object \n 2   GII                  926 non-null    float64\n 3   GII_change           926 non-null    float64\n 4   Deflator             926 non-null    float64\n 5   Nominal              926 non-null    float64\n 6   Population           926 non-null    float64\n 7   Real GDP             926 non-null    float64\n 8   Real GDP per Capita  926 non-null    float64\n 9   GDP_change           926 non-null    float64\ndtypes: float64(8), object(2)\nmemory usage: 79.6+ KB\n\n\n\n# Define independent and depenent variables\nX = combined_data['GDP_change']\ny = combined_data['GII_change']\n\n# Add a constant to indepdent variable to include an intercept\nX = sm.add_constant(X)\n\n# Fit a simple linear regresion model and print out the summary\nmodel = sm.OLS(y, X).fit()\nmodel.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\nGII_change\nR-squared:\n0.001\n\n\nModel:\nOLS\nAdj. R-squared:\n0.000\n\n\nMethod:\nLeast Squares\nF-statistic:\n1.256\n\n\nDate:\nMon, 10 Nov 2025\nProb (F-statistic):\n0.263\n\n\nTime:\n22:49:34\nLog-Likelihood:\n1326.5\n\n\nNo. Observations:\n926\nAIC:\n-2649.\n\n\nDf Residuals:\n924\nBIC:\n-2639.\n\n\nDf Model:\n1\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nconst\n-0.0258\n0.002\n-11.625\n0.000\n-0.030\n-0.021\n\n\nGDP_change\n0.0513\n0.046\n1.121\n0.263\n-0.039\n0.141\n\n\n\n\n\n\n\n\nOmnibus:\n812.205\nDurbin-Watson:\n1.517\n\n\nProb(Omnibus):\n0.000\nJarque-Bera (JB):\n118816.101\n\n\nSkew:\n-3.324\nProb(JB):\n0.00\n\n\nKurtosis:\n58.093\nCond. No.\n24.1\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n\nTime Series Analysis\nTime series analysis allows us to explore how the relationship between GII and GDP change vary across different time periods, accounting for lagged effects.\nHere was a quick summary of the result:\n\nBoth GII and GDP change time series were stationary.\nPast GII change values significantly influenced cuurent GII change values.\nVAR model had good model performance on forecasting future values based on historical data.\nChanges in GDP did not cause/precde the changes in GII.\n\n\nADF Test: Stationality Assumption Check\nWe wanted to use Augmented Dickey-Fuller (ADF) test to check whether a time series was stationary, which was the model assumption for many time series models.\nStationarity implied constant mean and variance over time, making it more predictable and stable for forecasting.\nBased on the ADF test output, both GII and GDP change time series were stationary. We proceeded to the time series modeling section.\n\n# Augmented Dickey-Fuller (ADF) test for stationarity check\n# Create melted datasets\ncombined_data_time = combined_data.melt(id_vars=['Time', 'Country'], \nvalue_vars=['GII_change','GDP_change'], \nvar_name = 'Change_Type', \nvalue_name = 'Value')\nGII = combined_data_time[(combined_data_time['Change_Type'] == 'GII_change')]                         \n\nGDP = combined_data_time[(combined_data_time['Change_Type'] == 'GDP_change')]\n\n# Stationary Check\ndef adf_test(series):\n    result = adfuller(series.dropna())\n    print(f'ADF Statistic: {result[0]}')\n    print(f'p-value: {result[1]}')\n    if result[1] &lt; 0.05:\n        print(\"Series is stationary\")\n    else:\n        print(\"Series is not stationary\")\n\n# Output the result\nadf_test(GII['Value'])\nadf_test(GDP['Value'])\n\nADF Statistic: -14.373733898266707\np-value: 9.400076395122232e-27\nSeries is stationary\nADF Statistic: -11.630733942799152\np-value: 2.265900981797003e-21\nSeries is stationary\n\n\n\n\nVAR model: Examine variables separately\nWe fitted a VAR (Vector Autoreression) model to see the relationship between GII and GDP change. VAR is particularly useful when dealing with multivariate time series data and allows us to examine the interdependence between variables.\nBased on summary, here were several interpretations we could make:\n\nWe used AIC as the criteria for model selection. Lower value suggests a better fit.\nGiven that we wanted to predict GII change, we focused on the first set “Results for equation GII_change.”\nPast GII_change values significantly influenced current GII_change, as shown in the small p-values of lags 1 and 2.\nLag 2 of GDP_change had a relatively low p-value but is not statistically significant.\n\n\n# Split the dataset into training and testing sets\nsplit_ratio = 0.7\nsplit_index = int(len(combined_data) * split_ratio)\n\n# Training set is used to fit the model\ntrain_data = combined_data.iloc[:split_index]\n\n# Testing set is used for validation\ntest_data = combined_data.iloc[split_index:]\n\nprint(f\"Training data: {train_data.shape}\")\nprint(f\"Test data: {test_data.shape}\")\n\nTraining data: (648, 10)\nTest data: (278, 10)\n\n\n\n# Fit a VAR model \ntime_model = VAR(train_data[['GII_change', 'GDP_change']])\ntime_model_fitted = time_model.fit(maxlags = 15, ic=\"aic\")\n\n# Print out the model summary\ntime_model_fitted.summary()\n\n  Summary of Regression Results   \n==================================\nModel:                         VAR\nMethod:                        OLS\nDate:           Mon, 10, Nov, 2025\nTime:                     22:49:34\n--------------------------------------------------------------------\nNo. of Equations:         2.00000    BIC:                   -12.0280\nNobs:                     641.000    HQIC:                  -12.1558\nLog likelihood:           2132.84    FPE:                4.84837e-06\nAIC:                     -12.2369    Det(Omega_mle):     4.62918e-06\n--------------------------------------------------------------------\nResults for equation GII_change\n================================================================================\n                   coefficient       std. error           t-stat            prob\n--------------------------------------------------------------------------------\nconst                -0.024001         0.004196           -5.721           0.000\nL1.GII_change         0.185009         0.039930            4.633           0.000\nL1.GDP_change        -0.026380         0.051164           -0.516           0.606\nL2.GII_change        -0.078497         0.040530           -1.937           0.053\nL2.GDP_change         0.023811         0.051238            0.465           0.642\nL3.GII_change         0.064899         0.040651            1.596           0.110\nL3.GDP_change         0.050686         0.051064            0.993           0.321\nL4.GII_change        -0.003574         0.040680           -0.088           0.930\nL4.GDP_change         0.072902         0.051130            1.426           0.154\nL5.GII_change         0.001834         0.040494            0.045           0.964\nL5.GDP_change        -0.001066         0.051175           -0.021           0.983\nL6.GII_change         0.072632         0.040469            1.795           0.073\nL6.GDP_change         0.002571         0.050984            0.050           0.960\nL7.GII_change         0.004002         0.039937            0.100           0.920\nL7.GDP_change         0.058530         0.050318            1.163           0.245\n================================================================================\n\nResults for equation GDP_change\n================================================================================\n                   coefficient       std. error           t-stat            prob\n--------------------------------------------------------------------------------\nconst                 0.012851         0.003255            3.948           0.000\nL1.GII_change        -0.013441         0.030978           -0.434           0.664\nL1.GDP_change         0.119880         0.039694            3.020           0.003\nL2.GII_change        -0.003697         0.031443           -0.118           0.906\nL2.GDP_change         0.065693         0.039751            1.653           0.098\nL3.GII_change        -0.008760         0.031537           -0.278           0.781\nL3.GDP_change         0.088826         0.039616            2.242           0.025\nL4.GII_change        -0.002124         0.031560           -0.067           0.946\nL4.GDP_change         0.090461         0.039667            2.280           0.023\nL5.GII_change         0.044080         0.031416            1.403           0.161\nL5.GDP_change         0.027609         0.039702            0.695           0.487\nL6.GII_change         0.063477         0.031396            2.022           0.043\nL6.GDP_change         0.071206         0.039554            1.800           0.072\nL7.GII_change        -0.090273         0.030983           -2.914           0.004\nL7.GDP_change         0.021749         0.039037            0.557           0.577\n================================================================================\n\nCorrelation matrix of residuals\n              GII_change  GDP_change\nGII_change      1.000000    0.015809\nGDP_change      0.015809    1.000000\n\n\n\n\n\nVAR Model: Forecasting\nWe applied the model learned above to the test data. Based on the plot, the forecast values seem to follow the actual data well, indicating a good model fit caputuring the underlying trends.\n\n# Number of steps to forecast (length of the test set)\nn_steps = len(test_data)\n\n# Get the last values from the training set for forecasting\nforecast_input = train_data[\n    ['GII_change', 'GDP_change']].values[-time_model_fitted.k_ar:]\n\n# Forecasting\nforecast = time_model_fitted.forecast(y=forecast_input, steps=n_steps)\n\n# Create a DataFrame for the forecasted values\nforecast_df = pd.DataFrame(forecast, index=test_data.index, \ncolumns=['GII_forecast', 'GDP_forecast'])\n\n# Ensure the index of the forecast_df matches the test_data index\nforecast_df.index = test_data.index\n\n\nplt.figure(figsize=(8, 6))\nplt.plot(train_data['GII_change'], label='Training GII', color='blue')\nplt.plot(test_data['GII_change'], label='Actual GII', color='orange')\nplt.plot(forecast_df['GII_forecast'], label='Forecasted GII', color='green')\nplt.title('GII Change Forecast vs Actual')\nplt.legend()\nplt.show()\n\nplt.figure(figsize=(8, 6))\nplt.plot(train_data['GDP_change'], label='Training GDP', color='blue')\nplt.plot(test_data['GDP_change'], label='Actual GDP', color='orange')\nplt.plot(forecast_df['GDP_forecast'], label='Forecasted GDP', color='green')\nplt.title('GDP Change Forecast vs Actual')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVAR Model: Model Performance\nLow values of both MAE and RMSE indicate good model performance with small average errors in predictions.\n\nmae_gii = mean_absolute_error(test_data['GII_change'], forecast_df['GII_forecast'])\nmae_gdp = mean_absolute_error(test_data['GDP_change'], forecast_df['GDP_forecast'])\n\nprint(f'Mean Absolute Error for GII: {mae_gii}')\nprint(f'Mean Absolute Error for GDP: {mae_gdp}')\n\nMean Absolute Error for GII: 0.029366879246630565\nMean Absolute Error for GDP: 0.02743613424611046\n\n\n\nrmse_gii = np.sqrt(mean_squared_error(test_data['GII_change'], \nforecast_df['GII_forecast']))\nrmse_gdp = np.sqrt(mean_squared_error(test_data['GDP_change'], \nforecast_df['GDP_forecast']))\n\nprint(f'RMSE for GII: {rmse_gii}')\nprint(f'RMSE for GDP: {rmse_gdp}')\n\nRMSE for GII: 0.06678650784425577\nRMSE for GDP: 0.03840528695249478\n\n\n\n\nVAR Model: Granger causality test\nGranger causality test evaluates whether one time series can predict another.\nBased on the output, the lowest p-value is when lag = 2. However, because p-value &gt; 0.05, we fail to reject the null hypothesis and conclude the GDP_change does not Granger-cause the GII_change.\n\n# Perform the Granger causality test\nmax_lag = 3\ntest_result = grangercausalitytests(train_data[['GII_change', 'GDP_change']], max_lag,\n verbose=True)\n\n\nGranger Causality\nnumber of lags (no zero) 1\nssr based F test:         F=0.0557  , p=0.8135  , df_denom=644, df_num=1\nssr based chi2 test:   chi2=0.0560  , p=0.8130  , df=1\nlikelihood ratio test: chi2=0.0560  , p=0.8130  , df=1\nparameter F test:         F=0.0557  , p=0.8135  , df_denom=644, df_num=1\n\nGranger Causality\nnumber of lags (no zero) 2\nssr based F test:         F=0.2860  , p=0.7514  , df_denom=641, df_num=2\nssr based chi2 test:   chi2=0.5764  , p=0.7496  , df=2\nlikelihood ratio test: chi2=0.5762  , p=0.7497  , df=2\nparameter F test:         F=0.2860  , p=0.7514  , df_denom=641, df_num=2\n\nGranger Causality\nnumber of lags (no zero) 3\nssr based F test:         F=0.8477  , p=0.4681  , df_denom=638, df_num=3\nssr based chi2 test:   chi2=2.5711  , p=0.4626  , df=3\nlikelihood ratio test: chi2=2.5660  , p=0.4635  , df=3\nparameter F test:         F=0.8477  , p=0.4681  , df_denom=638, df_num=3"
  },
  {
    "objectID": "docs/demo.html#conclusion",
    "href": "docs/demo.html#conclusion",
    "title": "Economic Growth and Gender Equality: An Analysis Using IMF Data",
    "section": "Conclusion",
    "text": "Conclusion\nIn wrapping up our analysis, we found no evidence to support a significant relationship between the Change in Real GDP per capita and the Change in the Gender Inequality Index (GII). This suggests that economic growth may not have a direct impact on gender equality. However, our findings open the door to questions for future research."
  },
  {
    "objectID": "docs/demo.html#future-directions",
    "href": "docs/demo.html#future-directions",
    "title": "Economic Growth and Gender Equality: An Analysis Using IMF Data",
    "section": "Future Directions",
    "text": "Future Directions\nFirst, we must consider what other factors might influence the relationship between GDP and GII change. The GII is a composite index, shaped by a myriad of social factors, including cultural norms, legal frameworks, and environmental shifts. Future studies could benefit from incorporating additional predictors into the analysis and exploring the interaction between economic growth and gender equality within specific country contexts.\nSecond, there’s potential to enhance the predictive power of our Vector Autoregression (VAR) time series model. While we established that GDP change does not cause GII change, our model performed well in forecasting trends for both variables independently. In practice, policymakers may want to forecast GII trends independently of GDP if they are implementing gender-focused policies. Future research could investigate time series modeling to further unravel the dynamics of GII and GDP changes.\nSo, as we wrap up this chapter, let’s keep our curiosity alive and our questions flowing. After all, every end is just a new beginning in the quest for knowledge!"
  },
  {
    "objectID": "docs/demo.html#about-the-author",
    "href": "docs/demo.html#about-the-author",
    "title": "Economic Growth and Gender Equality: An Analysis Using IMF Data",
    "section": "About the Author",
    "text": "About the Author\n\n\n\nHi there! My name is Jenny, and I’m a third-year student at University of California, Davis, double majoring in Statistics and Psychology. I’ve always been interested in becoming a data analyst working in tech, internet, or research industries. Interning at Promptly Technologies helped me learn a ton. A quick fun fact for me is that my MBTI is ISFJ (Defender)!\n\n  Email    LinkedIn    GitHub"
  },
  {
    "objectID": "docs/parameters.html",
    "href": "docs/parameters.html",
    "title": "Working with Parameters",
    "section": "",
    "text": "Once you have a database_id, it’s possible to make a call to imf_dataset to fetch the entire database:\n\nimport imfp\nimport pandas as pd\n\n# Set float format to 2 decimal places for pandas display output\npd.set_option('display.float_format', lambda x: '%.2f' % x)\n\n# Producer Price Index database\ndatabase_id = \"PPI\"\nimfp.imf_dataset(database_id)\n\nHowever, while this will succeed for a few small databases, it will fail for all of the larger ones. And even in the rare case when it succeeds, fetching an entire database can take a long time. You’re much better off supplying additional filter parameters to reduce the size of your request.\nRequests to databases available through the IMF API are complicated by the fact that each database uses a different set of parameters when making a request. You also have to have the list of valid input codes for each parameter. The imf_parameters function solves this problem. Use the function to obtain the full list of parameters and valid input codes for a given database."
  },
  {
    "objectID": "docs/parameters.html#filtering-imf-dataset-requests-with-parameters",
    "href": "docs/parameters.html#filtering-imf-dataset-requests-with-parameters",
    "title": "Working with Parameters",
    "section": "",
    "text": "Once you have a database_id, it’s possible to make a call to imf_dataset to fetch the entire database:\n\nimport imfp\nimport pandas as pd\n\n# Set float format to 2 decimal places for pandas display output\npd.set_option('display.float_format', lambda x: '%.2f' % x)\n\n# Producer Price Index database\ndatabase_id = \"PPI\"\nimfp.imf_dataset(database_id)\n\nHowever, while this will succeed for a few small databases, it will fail for all of the larger ones. And even in the rare case when it succeeds, fetching an entire database can take a long time. You’re much better off supplying additional filter parameters to reduce the size of your request.\nRequests to databases available through the IMF API are complicated by the fact that each database uses a different set of parameters when making a request. You also have to have the list of valid input codes for each parameter. The imf_parameters function solves this problem. Use the function to obtain the full list of parameters and valid input codes for a given database."
  },
  {
    "objectID": "docs/parameters.html#understanding-filter-parameters",
    "href": "docs/parameters.html#understanding-filter-parameters",
    "title": "Working with Parameters",
    "section": "Understanding Filter Parameters",
    "text": "Understanding Filter Parameters\nEach database available through the IMF API has its own set of parameters that can be used to filter and specify the data you want to retrieve.\nEach parameter will be a column in the data. Each row in the data will contain a value for that parameter. The parameter will always be a categorical variable, meaning that it can take only a limited set of values. We refer to these values as “input codes,” because you can input them in your API request to filter the data.\nWhat this means, though, is that before making an API request to retrieve data, you need to know what the available filtering parameters are for the database, and what codes you can use for filtering the data by each parameter.\nThere are two main functions for working with parameters:\n\nimf_parameters(): Get the full list of parameters and valid input codes for a database\nimf_parameter_defs(): Get text descriptions of what each parameter represents"
  },
  {
    "objectID": "docs/parameters.html#discovering-available-parameters",
    "href": "docs/parameters.html#discovering-available-parameters",
    "title": "Working with Parameters",
    "section": "Discovering Available Parameters",
    "text": "Discovering Available Parameters\nTo get started, you’ll need to know what parameters are available for your chosen database. Use imf_parameters() to get this information:\n\nimport imfp\n\n# Fetch list of valid parameters for the Producer Price Index database\nparams = imfp.imf_parameters(\"PPI\")\n\n# View the available parameter names\nparams.keys()\n\ndict_keys(['country', 'indicator', 'type_of_transformation', 'frequency'])\n\n\nThe function returns a dictionary of data frames.\nEach key in the dictionary corresponds to a parameter used in making requests from the database. The value for each key is a data frame with the following columns:\n\ninput_code: The valid codes you can use for that parameter\ndescription: A short text description of what each code represents\n\nFor example, to see the valid codes for the frequency parameter:\n\n# View the data frame of valid input codes for the frequency parameter\nparams['frequency']\n\n\n\n\n\n\n\n\ninput_code\ndescription\n\n\n\n\n0\nA\nAnnual\n\n\n1\nD\nDaily\n\n\n2\nM\nMonthly\n\n\n3\nQ\nQuarterly\n\n\n4\nS\nHalf-yearly, semester\n\n\n5\nW\nWeekly"
  },
  {
    "objectID": "docs/parameters.html#parameter-definitions",
    "href": "docs/parameters.html#parameter-definitions",
    "title": "Working with Parameters",
    "section": "Parameter Definitions",
    "text": "Parameter Definitions\nIf the parameter name is not self-explanatory, you can use the imf_parameter_defs() function to get a text description of what each parameter represents.\n\n# Get descriptions of what each parameter means\nparams_defs = imfp.imf_parameter_defs(\"PPI\")\n\nparams_defs\n\n\n\n\n\n\n\n\nparameter\ndescription\n\n\n\n\n0\ncountry\nCountry\n\n\n1\nindicator\nProducer Price Index (PPI) Indicator codelist\n\n\n2\ntype_of_transformation\nProducer Price Indexes (PPI) Type of Transform...\n\n\n3\nfrequency\nFrequency"
  },
  {
    "objectID": "docs/parameters.html#supplying-parameters",
    "href": "docs/parameters.html#supplying-parameters",
    "title": "Working with Parameters",
    "section": "Supplying Parameters",
    "text": "Supplying Parameters\n\nBasic Approach (Recommended for Most Users)\nTo make a request to fetch data from the IMF API, just call imf_dataset with the database ID and keyword arguments for each parameter, where the keyword argument name is the parameter name and the value is the list of codes you want.\nFor instance, on exploring the frequency parameter of the Producer Price Index database above, we found that the frequency can take one of three values: “A” for annual, “Q” for quarterly, and “M” for monthly. Thus, to request annual data, we can call imf_dataset with frequency = [\"A\"].\nHere’s a complete example that fetches annual producer prices from 2000 to 2015:\n\n# Example: Get annual prices\ndf = imfp.imf_dataset(\n    database_id=\"PPI\",\n    frequency=[\"A\"],  # Annual frequency\n    indicator=[\"PPI\"],  # Producer Price Index\n    start_year=2000,\n    end_year=2015\n)\n\n\n\nAdvanced Approaches\nFor more complex queries, there are two programmatic ways to supply parameters to imf_dataset. These approaches are particularly useful when you need to filter parameters based on their descriptions or when working with multiple parameter values.\n\n1. List Arguments with Parameter Filtering\nThis approach uses string matching to find the correct parameter codes before passing them to imf_dataset:\n\n# Fetch the input code column of the frequency parameter...\nselected_frequency = list(\n    params['frequency']['input_code'][\n        # ...where the description contains \"Annual\"\n        params['frequency']['description'].str.contains(\"Annual\")\n    ]\n)\n\n# Fetch the input code column of the unit_measure parameter...\nselected_type_of_transformation = list(\n    params['type_of_transformation']['input_code'][\n        # ...where the description contains \"Index\"\n        params['type_of_transformation']['description'].str.contains(\"Index\")\n    ]\n)\n\n# Request data from the API using the filtered parameter code lists\ndf = imfp.imf_dataset(\n    database_id=\"PPI\",\n    frequency=selected_frequency,\n    type_of_transformation=selected_type_of_transformation,\n    start_year=2000,\n    end_year=2015\n)\n\ndf.head()\n\n\n\n\n\n\n\n\ncountry\nindicator\ntype_of_transformation\nfrequency\ntime_period\nobs_value\n\n\n\n\n0\nAGO\nWPI\nIX\nA\n2008\n80.099924\n\n\n1\nAGO\nWPI\nIX\nA\n2009\n88.676029\n\n\n2\nAGO\nWPI\nIX\nA\n2010\n100.000000\n\n\n3\nAGO\nWPI\nIX\nA\n2011\n111.889914\n\n\n4\nAGO\nWPI\nIX\nA\n2012\n117.840781\n\n\n\n\n\n\n\n\n\n2. Parameters Dictionary Approach\nThis approach modifies the parameters dictionary directly and passes the entire filtered dictionary to imf_dataset as a single parameters keyword argument. This is more concise but requires understanding how the parameters dictionary works:\n\n# Copy the params dictionary\nmodified_params = params.copy()\n\n# Overwrite the data frame for each parameter in the dictionary with filtered rows\nmodified_params['frequency'] = params['frequency'][\n    # ...where the input code description for frequency contains \"Annual\"\n    params['frequency']['description'].str.contains(\"Annual\")\n]\nmodified_params['type_of_transformation'] = params['type_of_transformation'][\n    # ...where the input code description for type_of_transformation contains \"Index\"\n    params['type_of_transformation']['description'].str.contains(\"Index\")\n]\n\n# Pass the modified dictionary to imf_dataset\ndf = imfp.imf_dataset(\n    database_id=\"PPI\",\n    parameters=modified_params,\n    start_year=2000,\n    end_year=2015\n)\n\ndf.head()\n\n\n\n\n\n\n\n\ncountry\nindicator\ntype_of_transformation\nfrequency\ntime_period\nobs_value\n\n\n\n\n0\nAGO\nWPI\nIX\nA\n2008\n80.099924\n\n\n1\nAGO\nWPI\nIX\nA\n2009\n88.676029\n\n\n2\nAGO\nWPI\nIX\nA\n2010\n100.000000\n\n\n3\nAGO\nWPI\nIX\nA\n2011\n111.889914\n\n\n4\nAGO\nWPI\nIX\nA\n2012\n117.840781\n\n\n\n\n\n\n\nNote that when using the parameters dictionary approach, you cannot combine it with individual parameter arguments. If you supply a parameters argument, any other keyword arguments for individual parameters will be ignored."
  },
  {
    "objectID": "docs/usage.html",
    "href": "docs/usage.html",
    "title": "Suggestions for Usage",
    "section": "",
    "text": "The only way to be certain whether data is available for a given set of parameters is to make a request to the API and see if it succeeds. If you get an empty data frame, try a less restrictive version of your request."
  },
  {
    "objectID": "docs/usage.html#determining-data-availability",
    "href": "docs/usage.html#determining-data-availability",
    "title": "Suggestions for Usage",
    "section": "",
    "text": "The only way to be certain whether data is available for a given set of parameters is to make a request to the API and see if it succeeds. If you get an empty data frame, try a less restrictive version of your request."
  },
  {
    "objectID": "docs/usage.html#working-with-large-data-frames",
    "href": "docs/usage.html#working-with-large-data-frames",
    "title": "Suggestions for Usage",
    "section": "Working with Large Data Frames",
    "text": "Working with Large Data Frames\n\nInspecting Data\nimfp outputs data in pandas DataFrames, so you will want to use the pandas package for its functions for viewing and manipulating this object type.\nFor large datasets, you can use the pandas library’s info() method to get a quick summary of the data frame, including the number of rows and columns, the count of non-missing values, the column names, and the data types.\n\nimport imfp\nimport pandas as pd\n\n# Set float format to 2 decimal places for pandas display output\npd.set_option('display.float_format', lambda x: '%.2f' % x)\n\ndf: pd.DataFrame = imfp.imf_dataset(\n    database_id=\"PCPS\",\n    indicator=[\"PCOAL\"],\n    data_transformation=[\"IX\"]\n)\n\n# Quick summary of DataFrame\ndf.info()\n\n/home/runner/work/imfp/imfp/imfp/data.py:582: UserWarning: ['IX'] not valid value(s) for data_transformation and will be ignored. Use imf_parameters('PCPS') to get valid parameters.\n  warn(\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 1722 entries, 0 to 1721\nData columns (total 6 columns):\n #   Column               Non-Null Count  Dtype  \n---  ------               --------------  -----  \n 0   country              1722 non-null   object \n 1   indicator            1722 non-null   object \n 2   data_transformation  1722 non-null   object \n 3   frequency            1722 non-null   object \n 4   time_period          1722 non-null   object \n 5   obs_value            1722 non-null   float64\ndtypes: float64(1), object(5)\nmemory usage: 80.8+ KB\n\n\nAlternatively, you can use the head() method to view the first 5 rows of the data frame.\n\n# View first 5 rows of DataFrame\ndf.head()\n\n\n\n\n\n\n\n\ncountry\nindicator\ndata_transformation\nfrequency\ntime_period\nobs_value\n\n\n\n\n0\nG001\nPCOAL\nINDEX\nA\n1992\n49.89\n\n\n1\nG001\nPCOAL\nINDEX\nA\n1993\n43.28\n\n\n2\nG001\nPCOAL\nINDEX\nA\n1994\n45.21\n\n\n3\nG001\nPCOAL\nINDEX\nA\n1995\n55.43\n\n\n4\nG001\nPCOAL\nINDEX\nA\n1996\n53.18\n\n\n\n\n\n\n\n\n\nCleaning Data\n\nNumeric Conversion\nAll data is returned from the IMF API as a text (object) data type, so you will want to cast numeric columns to numeric.\n\n# Numeric columns\nnumeric_cols = [\"obs_value\"]\n\n# Cast numeric columns\ndf[numeric_cols] = df[numeric_cols].apply(pd.to_numeric)\n\n/tmp/ipykernel_7570/2877006961.py:5: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\nYou are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\nA typical example is when you are setting values in a column of a DataFrame, like:\n\ndf[\"col\"][row_indexer] = value\n\nUse `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n  df[numeric_cols] = df[numeric_cols].apply(pd.to_numeric)\n\n\n\n\nCategorical Conversion\nYou can also convert string columns to categorical types for better memory usage.\n\n# Convert categorical columns like ref_area and indicator to category type\ncategorical_cols = [\n  \"frequency\",\n  \"country\",\n  \"indicator\"\n]\n\ndf[categorical_cols] = df[categorical_cols].astype(\"category\")\n\n/tmp/ipykernel_7570/1114720700.py:8: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\nYou are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\nA typical example is when you are setting values in a column of a DataFrame, like:\n\ndf[\"col\"][row_indexer] = value\n\nUse `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n  df[categorical_cols] = df[categorical_cols].astype(\"category\")\n\n\n\n\nNA Removal\nAfter conversion, you may want to drop any rows with missing values.\n\n# Drop rows with missing values\ndf = df.dropna()\n\n\n\nTime Period Conversion\nThe time_period column can be more difficult to work with, because it may be differently formatted depending on the frequency of the data.\nAnnual data will be formatted as a four-digit year, such as “2000”, which can be trivially converted to numeric.\nHowever, quarterly data will be formatted as “2000-Q1”, monthly data will be formatted like “2000-M01”, etc.\nYou can use the pandas library’s to_datetime() method with the format=\"mixed\" argument to convert this column to a datetime object in a format-agnostic way:\n\n# Convert time_period to datetime\ndf[\"datetime\"] = pd.to_datetime(df[\"time_period\"], format=\"mixed\")\ndf[[\"frequency\", \"datetime\"]].head()\n\n/tmp/ipykernel_7570/3082768046.py:2: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\nYou are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\nA typical example is when you are setting values in a column of a DataFrame, like:\n\ndf[\"col\"][row_indexer] = value\n\nUse `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n  df[\"datetime\"] = pd.to_datetime(df[\"time_period\"], format=\"mixed\")\n\n\n\n\n\n\n\n\n\nfrequency\ndatetime\n\n\n\n\n0\nA\n1992-01-01\n\n\n1\nA\n1993-01-01\n\n\n2\nA\n1994-01-01\n\n\n3\nA\n1995-01-01\n\n\n4\nA\n1996-01-01\n\n\n\n\n\n\n\nAlternatively, you can split the time_period column into separate columns for year, quarter, and month, and then convert each to a numeric value:\n\n# Split time_period into separate columns\ndf[\"year\"] = df[\"time_period\"].str.extract(r\"(\\d{4})\")[0]\ndf[\"quarter\"] = df[\"time_period\"].str.extract(r\"[Q](\\d{1})\")[0]\ndf[\"month\"] = df[\"time_period\"].str.extract(r\"[M](\\d{2})\")[0]\n\n# Convert year, quarter, and month to numeric\ndf[\"year\"] = pd.to_numeric(df[\"year\"])\ndf[\"quarter\"] = pd.to_numeric(df[\"quarter\"])\ndf[\"month\"] = pd.to_numeric(df[\"month\"])\n\n# Return head for non-na months\ndf[[\"time_period\", \"year\", \"quarter\", \"month\"]].dropna(subset=[\"month\"]).head()\n\n/tmp/ipykernel_7570/39852174.py:2: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\nYou are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\nA typical example is when you are setting values in a column of a DataFrame, like:\n\ndf[\"col\"][row_indexer] = value\n\nUse `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n  df[\"year\"] = df[\"time_period\"].str.extract(r\"(\\d{4})\")[0]\n/tmp/ipykernel_7570/39852174.py:3: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\nYou are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\nA typical example is when you are setting values in a column of a DataFrame, like:\n\ndf[\"col\"][row_indexer] = value\n\nUse `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n  df[\"quarter\"] = df[\"time_period\"].str.extract(r\"[Q](\\d{1})\")[0]\n/tmp/ipykernel_7570/39852174.py:4: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\nYou are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\nA typical example is when you are setting values in a column of a DataFrame, like:\n\ndf[\"col\"][row_indexer] = value\n\nUse `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n  df[\"month\"] = df[\"time_period\"].str.extract(r\"[M](\\d{2})\")[0]\n/tmp/ipykernel_7570/39852174.py:7: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\nYou are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\nA typical example is when you are setting values in a column of a DataFrame, like:\n\ndf[\"col\"][row_indexer] = value\n\nUse `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n  df[\"year\"] = pd.to_numeric(df[\"year\"])\n/tmp/ipykernel_7570/39852174.py:8: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\nYou are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\nA typical example is when you are setting values in a column of a DataFrame, like:\n\ndf[\"col\"][row_indexer] = value\n\nUse `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n  df[\"quarter\"] = pd.to_numeric(df[\"quarter\"])\n/tmp/ipykernel_7570/39852174.py:9: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\nYou are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\nA typical example is when you are setting values in a column of a DataFrame, like:\n\ndf[\"col\"][row_indexer] = value\n\nUse `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n  df[\"month\"] = pd.to_numeric(df[\"month\"])\n\n\n\n\n\n\n\n\n\ntime_period\nyear\nquarter\nmonth\n\n\n\n\n33\n1992-M01\n1992\nNaN\n1.00\n\n\n34\n1992-M02\n1992\nNaN\n2.00\n\n\n35\n1992-M03\n1992\nNaN\n3.00\n\n\n36\n1992-M04\n1992\nNaN\n4.00\n\n\n37\n1992-M05\n1992\nNaN\n5.00\n\n\n\n\n\n\n\n\n\n\nSummarizing Data\nAfter converting columns to numeric, you can use the describe() function to get a quick summary of the statistical properties of these, including the count of rows, the mean, the standard deviation, the minimum and maximum values, and the quartiles.\n\n# Statistical summary\ndf.describe()\n\n\n\n\n\n\n\n\nobs_value\ndatetime\nyear\nquarter\nmonth\n\n\n\n\ncount\n1722.00\n1722\n1722.00\n405.00\n1218.00\n\n\nmean\n41.76\n2008-06-21 19:04:02.907665408\n2008.39\n2.49\n6.48\n\n\nmin\n-69.21\n1992-01-01 00:00:00\n1992.00\n1.00\n1.00\n\n\n25%\n-3.97\n2000-01-01 00:00:06\n2000.00\n1.00\n3.00\n\n\n50%\n9.01\n2008-05-16 12:00:00\n2008.00\n2.00\n6.00\n\n\n75%\n64.32\n2017-01-01 00:00:04\n2017.00\n3.00\n9.00\n\n\nmax\n577.58\n2025-07-01 00:00:00\n2025.00\n4.00\n12.00\n\n\nstd\n77.21\nNaN\n9.75\n1.12\n3.44\n\n\n\n\n\n\n\n\n\nViewing Data\nFor large data frames, it can be useful to view the data in a browser window. To facilitate this, you can define a View() function as follows. This function will save the data frame to a temporary HTML file and open it in your default web browser.\n\nimport tempfile\nimport webbrowser\n\n# Define a simple function to view data frame in a browser window\ndef View(df: pd.DataFrame):\n    html = df.to_html()\n    with tempfile.NamedTemporaryFile('w', \n    delete=False, suffix='.html') as f:\n        url = 'file://' + f.name\n        f.write(html)\n    webbrowser.open(url)\n\n# Call the function\nView(df)"
  },
  {
    "objectID": "docs/usage.html#common-data-transformations",
    "href": "docs/usage.html#common-data-transformations",
    "title": "Suggestions for Usage",
    "section": "Common Data Transformations",
    "text": "Common Data Transformations\nThe World Economic Outlook (WEO) and Consumer Price Index (CPI) databases provide key macroeconomic aggregates that are frequently needed when working with other IMF datasets. Here, we’ll demonstrate how to use three fundamental indicators—GDP, price deflators, and population statistics—to transform your data.\nThese transformations are essential for:\n\nConverting nominal to real dollar values\nCalculating per capita metrics\nHarmonizing data across different frequencies\nAdjusting for different unit scales\n\nFor a complete, end-to-end example of these transformations in a real analysis workflow, see Jenny Xu’s superb demo notebook.\n\nFetching Adjusters\nFirst, let’s retrieve the key adjustment variables:\n\n# Fetch GDP Deflator (Index, Quarterly)\ndeflator = imfp.imf_dataset(\n    database_id=\"QNEA\",\n    indicator=\"B1GQ\",\n    price_type=\"PD\",  # Price deflator\n    type_of_transformation=\"IX\",  # Index\n    frequency=\"Q\",\n    start_year=2010\n)\n\n# Fetch Population Estimates (Annual)\npopulation = imfp.imf_dataset(\n    database_id=\"WEO\",\n    indicator=\"LP\",\n    frequency=\"A\",\n    start_year=2010\n)\n\n# Fetch Exchange Rate (Quarterly)\nexchange_rate = imfp.imf_dataset(\n    database_id=\"ER\", \n    indicator=\"XDC_USD\",  # Domestic currency per USD\n    frequency=\"Q\",\n    start_year=2010\n)\n\n/home/runner/work/imfp/imfp/imfp/data.py:748: UserWarning: Agency IMF.RES does not support time filters; time window will be ignored.\n  warn(\n\n\nWe’ll also retrieve a nominal GDP series to be adjusted:\n\n# Fetch Nominal GDP (Domestic currency, annual)\nnominal_gdp = imfp.imf_dataset(\n    database_id=\"ANEA\",\n    indicator=\"B1GQ\",\n    price_type=\"V\",  # Current prices\n    type_of_transformation=\"XDC\",  # Domestic currency\n    frequency=\"A\",\n    start_year=2010\n)\n\nKey Indicators:\n\nQNEA (Quarterly National Economic Accounts): B1GQ with price_type=\"PD\" and type_of_transformation=\"IX\" for GDP deflator index\nWEO (World Economic Outlook): LP for population estimates\nER (Exchange Rates): XDC_USD for exchange rate (domestic currency per USD)\nANEA (Annual National Economic Accounts): B1GQ with price_type=\"V\" and type_of_transformation=\"XDC\" for nominal GDP in domestic currency\n\n\n\n\n\n\n\nNoteDatabase Changes\n\n\n\nThe IMF has updated their API structure. The former IFS (International Financial Statistics) database, which provided a central point of access to these adjusters, has been discontinued and replaced with more specialized databases:\n\nANEA/QNEA: National Economic Accounts data (annual and quarterly)\nWEO: World Economic Outlook data including population\nER: Exchange rate data\nCPI: Consumer Price Index data\nMFS_CBS: Monetary and Financial Statistics, Central Bank data\n\nUse imf_databases() to see all available databases and imf_parameters(database_id) to explore their indicators.\n\n\n\n\nAlternative: Using CPI for Price Adjustments\nIf you prefer to use the Consumer Price Index instead of the GDP deflator:\n\n# Fetch CPI (All Items, Index)\ncpi = imfp.imf_dataset(\n    database_id=\"CPI\",\n    index_type=\"CPI\",\n    coicop_1999=\"_T\",  # All Items\n    type_of_transformation=\"IX\",  # Index\n    frequency=\"Q\",\n    start_year=2010\n)\n\n\n\nHarmonizing Frequencies\nWhen working with data of different frequencies, you’ll often need to harmonize them. For example, population and national GDP are available at an annual frequency, while the GDP deflator and exchange rates can only be obtained at a monthly or quarterly frequency. There are two common approaches:\n\nUsing Q4 values: This approach is often used for stock variables (measurements taken at a point in time) and when you want to align with end-of-year values:\n\n\n# Keep only Q4 observations for annual comparisons\ndeflator = deflator[deflator['time_period'].str.contains(\"Q4\")]\nexchange_rate = exchange_rate[exchange_rate['time_period'].str.contains(\"Q4\")]\n\n# Extract just the year from the time period for Q4 data\ndeflator['time_period'] = deflator['time_period'].str[:4]\nexchange_rate['time_period'] = exchange_rate['time_period'].str[:4]\n\n/tmp/ipykernel_7570/2718242792.py:6: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\nYou are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\nA typical example is when you are setting values in a column of a DataFrame, like:\n\ndf[\"col\"][row_indexer] = value\n\nUse `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n  deflator['time_period'] = deflator['time_period'].str[:4]\n/tmp/ipykernel_7570/2718242792.py:7: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\nYou are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\nA typical example is when you are setting values in a column of a DataFrame, like:\n\ndf[\"col\"][row_indexer] = value\n\nUse `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n  exchange_rate['time_period'] = exchange_rate['time_period'].str[:4]\n\n\n\nCalculating annual averages: This approach is more appropriate for flow variables (measurements over a period) and when you want to smooth out seasonal variations:\n\n\n# Alternative: Calculate annual averages\ndeflator = deflator.groupby(\n    ['country', deflator['time_period']], \n    as_index=False\n).agg({\n    'obs_value': 'mean'\n})\n\nChoose the appropriate method based on your specific analysis needs and the economic meaning of your variables.\n\n\nMerging Datasets\nWe can combine the datasets using pd.DataFrame.merge() with country and time_period as keys:\n\nmerged = (\n    nominal_gdp.merge(\n        deflator,\n        on=['country', 'time_period'],\n        suffixes=('_gdp', '_deflator')\n    )\n    .merge(\n        population,\n        on=['country', 'time_period']\n    )\n    .merge(\n        exchange_rate,\n        on=['country', 'time_period'],\n        suffixes=('_population', '_exchange_rate')\n    )\n)\n\n\n\nCalculating Real Values\nWith the merged dataset, we can now calculate real GDP and per capita values:\n\n# Convert nominal to real GDP\nmerged['real_gdp'] = (\n    (merged['obs_value_gdp'] / merged['obs_value_deflator']) * 100\n)\n\n# Calculate per capita values (using population obs_value)\nmerged['real_gdp_per_capita'] = merged['real_gdp'] / merged['obs_value_population']\n\n# Display the first 5 rows of the transformed data\nmerged[['country', 'time_period', 'real_gdp', 'real_gdp_per_capita']].head()\n\n/tmp/ipykernel_7570/35632758.py:2: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\nYou are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\nA typical example is when you are setting values in a column of a DataFrame, like:\n\ndf[\"col\"][row_indexer] = value\n\nUse `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n  merged['real_gdp'] = (\n/tmp/ipykernel_7570/35632758.py:7: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\nYou are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\nA typical example is when you are setting values in a column of a DataFrame, like:\n\ndf[\"col\"][row_indexer] = value\n\nUse `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n  merged['real_gdp_per_capita'] = merged['real_gdp'] / merged['obs_value_population']\n\n\n\n\n\n\n\n\n\ncountry\ntime_period\nreal_gdp\nreal_gdp_per_capita\n\n\n\n\n0\nALB\n2011\n1266392354394.81\n435935.41\n\n\n1\nALB\n2011\n1266392354394.81\n435935.41\n\n\n2\nALB\n2012\n1302994927918.21\n449308.60\n\n\n3\nALB\n2012\n1302994927918.21\n449308.60\n\n\n4\nALB\n2013\n1327471211207.70\n458539.28\n\n\n\n\n\n\n\n\n\nExchange Rate Adjustment\nNote that this result is still in the domestic currency of the country. If you need to convert to a common currency, you can use the exchange rate data from the ER (Exchange Rates) database.\n\n# Because 'obs_value_exchange_rate' is local-currency-per-USD,\n# dividing local-currency real GDP by it yields GDP in USD.\nmerged[\"real_gdp_usd\"] = (\n    merged[\"real_gdp\"] / merged[\"obs_value_exchange_rate\"]\n)\n\n# (Optional) real GDP per capita in USD\nmerged[\"real_gdp_usd_per_capita\"] = (\n    merged[\"real_gdp_usd\"] / merged[\"obs_value_population\"]\n)\n\n# Inspect results\nmerged[[\"time_period\",\"country\",\"real_gdp\",\"real_gdp_usd\",\"real_gdp_usd_per_capita\"]].head()\n\n/tmp/ipykernel_7570/400048459.py:3: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\nYou are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\nA typical example is when you are setting values in a column of a DataFrame, like:\n\ndf[\"col\"][row_indexer] = value\n\nUse `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n  merged[\"real_gdp_usd\"] = (\n/tmp/ipykernel_7570/400048459.py:8: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\nYou are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\nA typical example is when you are setting values in a column of a DataFrame, like:\n\ndf[\"col\"][row_indexer] = value\n\nUse `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n  merged[\"real_gdp_usd_per_capita\"] = (\n\n\n\n\n\n\n\n\n\ntime_period\ncountry\nreal_gdp\nreal_gdp_usd\nreal_gdp_usd_per_capita\n\n\n\n\n0\n2011\nALB\n1266392354394.81\n11776012222.38\n4053.70\n\n\n1\n2011\nALB\n1266392354394.81\n12190133681.53\n4196.26\n\n\n2\n2012\nALB\n1302994927918.21\n12309824543.39\n4244.77\n\n\n3\n2012\nALB\n1302994927918.21\n12088272826.03\n4168.37\n\n\n4\n2013\nALB\n1327471211207.70\n13032311125.15\n4501.66"
  }
]